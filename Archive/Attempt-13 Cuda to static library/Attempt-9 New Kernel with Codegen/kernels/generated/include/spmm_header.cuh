#define SPMM_KERNEL_COUNT 16
        
    typedef void (*gfusedMM_spmm_t) ( const char transa, const INDEXTYPE m, 
      const INDEXTYPE n, const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);void gfusedMM_bl1_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl2_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl3_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl4_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl5_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl6_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl7_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl8_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl9_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl10_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl11_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl12_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl13_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl14_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl15_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl16_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl17_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl18_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl19_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl20_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl21_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl22_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl23_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl24_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl25_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl26_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl27_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl28_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl29_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl30_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl31_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl32_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl33_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl34_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl35_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl36_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl37_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl38_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl39_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl40_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl41_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl42_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl43_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl44_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl45_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl46_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl47_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl48_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl49_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl50_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl51_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl52_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl53_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl54_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl55_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl56_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl57_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl58_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl59_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl60_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl61_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl62_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl63_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl64_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl65_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl66_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl67_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl68_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl69_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl70_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl71_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl72_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl73_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl74_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl75_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl76_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl77_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl78_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl79_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl80_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl81_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl82_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl83_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl84_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl85_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl86_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl87_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl88_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl89_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl90_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl91_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl92_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl93_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl94_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl95_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl96_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl97_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl98_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl99_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl100_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl101_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl102_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl103_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl104_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl105_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl106_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl107_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl108_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl109_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl110_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl111_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl112_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl113_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl114_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl115_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl116_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl117_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl118_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl119_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl120_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl121_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl122_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl123_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl124_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl125_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl126_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl127_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl128_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl129_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl130_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl131_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl132_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl133_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl134_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl135_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl136_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl137_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl138_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl139_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl140_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl141_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl142_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl143_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl144_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl145_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl146_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl147_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl148_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl149_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl150_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl151_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl152_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl153_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl154_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl155_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl156_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl157_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl158_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl159_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl160_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl161_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl162_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl163_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl164_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl165_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl166_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl167_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl168_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl169_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl170_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl171_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl172_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl173_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl174_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl175_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl176_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl177_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl178_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl179_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl180_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl181_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl182_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl183_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl184_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl185_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl186_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl187_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl188_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl189_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl190_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl191_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl192_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl193_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl194_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl195_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl196_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl197_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl198_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl199_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl200_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl201_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl202_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl203_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl204_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl205_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl206_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl207_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl208_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl209_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl210_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl211_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl212_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl213_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl214_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl215_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl216_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl217_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl218_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl219_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl220_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl221_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl222_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl223_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl224_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl225_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl226_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl227_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl228_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl229_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl230_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl231_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl232_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl233_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl234_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl235_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl236_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl237_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl238_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl239_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl240_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl241_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl242_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl243_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl244_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl245_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl246_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl247_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl248_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl249_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl250_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl251_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl252_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl253_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl254_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl255_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl256_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl257_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl258_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl259_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl260_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl261_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl262_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl263_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl264_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl265_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl266_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl267_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl268_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl269_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl270_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl271_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl272_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl273_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl274_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl275_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl276_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl277_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl278_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl279_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl280_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl281_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl282_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl283_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl284_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl285_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl286_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl287_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl288_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl289_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl290_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl291_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl292_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl293_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl294_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl295_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl296_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl297_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl298_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl299_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl300_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl301_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl302_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl303_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl304_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl305_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl306_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl307_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl308_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl309_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl310_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl311_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl312_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl313_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl314_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl315_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl316_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl317_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl318_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl319_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl320_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl321_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl322_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl323_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl324_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl325_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl326_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl327_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl328_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl329_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl330_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl331_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl332_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl333_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl334_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl335_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl336_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl337_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl338_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl339_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl340_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl341_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl342_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl343_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl344_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl345_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl346_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl347_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl348_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl349_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl350_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl351_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl352_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl353_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl354_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl355_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl356_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl357_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl358_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl359_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl360_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl361_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl362_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl363_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl364_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl365_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl366_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl367_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl368_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl369_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl370_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl371_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl372_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl373_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl374_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl375_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl376_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl377_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl378_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl379_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl380_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl381_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl382_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl383_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl384_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl385_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl386_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl387_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl388_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl389_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl390_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl391_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl392_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl393_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl394_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl395_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl396_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl397_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl398_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl399_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl400_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl401_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl402_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl403_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl404_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl405_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl406_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl407_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl408_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl409_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl410_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl411_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl412_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl413_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl414_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl415_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl416_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl417_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl418_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl419_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl420_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl421_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl422_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl423_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl424_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl425_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl426_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl427_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl428_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl429_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl430_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl431_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl432_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl433_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl434_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl435_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl436_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl437_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl438_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl439_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl440_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl441_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl442_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl443_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl444_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl445_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl446_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl447_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl448_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl449_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl450_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl451_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl452_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl453_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl454_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl455_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl456_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl457_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl458_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl459_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl460_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl461_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl462_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl463_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl464_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl465_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl466_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl467_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl468_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl469_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl470_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl471_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl472_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl473_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl474_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl475_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl476_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl477_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl478_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl479_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl480_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl481_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl482_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl483_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl484_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl485_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl486_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl487_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl488_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl489_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl490_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl491_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl492_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl493_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl494_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl495_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl496_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl497_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl498_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl499_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl500_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl501_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl502_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl503_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl504_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl505_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl506_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl507_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl508_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl509_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl510_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl511_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl512_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl513_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl514_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl515_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl516_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl517_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl518_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl519_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl520_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl521_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl522_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl523_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl524_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl525_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl526_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl527_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl528_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl529_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl530_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl531_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl532_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl533_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl534_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl535_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl536_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl537_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl538_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl539_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl540_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl541_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl542_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl543_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl544_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl545_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl546_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl547_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl548_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl549_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl550_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl551_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl552_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl553_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl554_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl555_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl556_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl557_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl558_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl559_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl560_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl561_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl562_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl563_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl564_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl565_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl566_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl567_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl568_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl569_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl570_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl571_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl572_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl573_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl574_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl575_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl576_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl577_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl578_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl579_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl580_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl581_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl582_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl583_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl584_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl585_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl586_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl587_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl588_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl589_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl590_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl591_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl592_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl593_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl594_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl595_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl596_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl597_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl598_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl599_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl600_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl601_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl602_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl603_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl604_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl605_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl606_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl607_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl608_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl609_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl610_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl611_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl612_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl613_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl614_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl615_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl616_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl617_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl618_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl619_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl620_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl621_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl622_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl623_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl624_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl625_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl626_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl627_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl628_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl629_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl630_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl631_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl632_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl633_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl634_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl635_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl636_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl637_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl638_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl639_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl640_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl641_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl642_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl643_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl644_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl645_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl646_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl647_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl648_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl649_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl650_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl651_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl652_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl653_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl654_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl655_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl656_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl657_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl658_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl659_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl660_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl661_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl662_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl663_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl664_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl665_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl666_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl667_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl668_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl669_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl670_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl671_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl672_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl673_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl674_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl675_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl676_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl677_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl678_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl679_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl680_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl681_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl682_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl683_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl684_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl685_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl686_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl687_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl688_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl689_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl690_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl691_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl692_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl693_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl694_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl695_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl696_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl697_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl698_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl699_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl700_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl701_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl702_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl703_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl704_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl705_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl706_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl707_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl708_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl709_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl710_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl711_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl712_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl713_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl714_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl715_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl716_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl717_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl718_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl719_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl720_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl721_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl722_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl723_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl724_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl725_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl726_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl727_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl728_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl729_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl730_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl731_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl732_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl733_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl734_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl735_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl736_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl737_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl738_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl739_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl740_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl741_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl742_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl743_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl744_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl745_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl746_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl747_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl748_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl749_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl750_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl751_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl752_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl753_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl754_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl755_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl756_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl757_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl758_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl759_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl760_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl761_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl762_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl763_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl764_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl765_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl766_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl767_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl768_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl769_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl770_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl771_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl772_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl773_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl774_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl775_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl776_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl777_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl778_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl779_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl780_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl781_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl782_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl783_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl784_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl785_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl786_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl787_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl788_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl789_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl790_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl791_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl792_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl793_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl794_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl795_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl796_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl797_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl798_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl799_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl800_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl801_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl802_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl803_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl804_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl805_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl806_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl807_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl808_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl809_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl810_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl811_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl812_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl813_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl814_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl815_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl816_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl817_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl818_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl819_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl820_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl821_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl822_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl823_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl824_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl825_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl826_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl827_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl828_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl829_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl830_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl831_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl832_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl833_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl834_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl835_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl836_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl837_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl838_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl839_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl840_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl841_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl842_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl843_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl844_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl845_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl846_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl847_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl848_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl849_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl850_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl851_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl852_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl853_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl854_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl855_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl856_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl857_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl858_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl859_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl860_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl861_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl862_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl863_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl864_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl865_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl866_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl867_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl868_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl869_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl870_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl871_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl872_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl873_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl874_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl875_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl876_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl877_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl878_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl879_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl880_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl881_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl882_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl883_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl884_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl885_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl886_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl887_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl888_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl889_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl890_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl891_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl892_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl893_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl894_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl895_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl896_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl897_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl898_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl899_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl900_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl901_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl902_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl903_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl904_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl905_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl906_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl907_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl908_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl909_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl910_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl911_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl912_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl913_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl914_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl915_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl916_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl917_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl918_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl919_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl920_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl921_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl922_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl923_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl924_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl925_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl926_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl927_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl928_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl929_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl930_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl931_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl932_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl933_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl934_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl935_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl936_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl937_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl938_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl939_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl940_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl941_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl942_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl943_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl944_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl945_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl946_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl947_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl948_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl949_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl950_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl951_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl952_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl953_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl954_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl955_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl956_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl957_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl958_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl959_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl960_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl961_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl962_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl963_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl964_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl965_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl966_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl967_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl968_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl969_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl970_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl971_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl972_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl973_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl974_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl975_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl976_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl977_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl978_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl979_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl980_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl981_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl982_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl983_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl984_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl985_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl986_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl987_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl988_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl989_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl990_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl991_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl992_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl993_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl994_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl995_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl996_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl997_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl998_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl999_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl1000_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl1001_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl1002_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl1003_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl1004_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl1005_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl1006_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl1007_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl1008_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl1009_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl1010_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl1011_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl1012_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl1013_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl1014_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl1015_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl1016_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl1017_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl1018_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl1019_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl1020_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl1021_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl1022_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl1023_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);

void gfusedMM_bl1024_spmm_csr (const char transa, const INDEXTYPE m, const INDEXTYPE n, 
      const INDEXTYPE k,const float alpha, const INDEXTYPE nnz, 
      const INDEXTYPE rows, const INDEXTYPE cols, const float *val, const INDEXTYPE *indx, 
      const INDEXTYPE *pntrb, const INDEXTYPE *pntre, const float *A, 
      const INDEXTYPE lda, const float *B, const INDEXTYPE ldb, 
      const float beta, float *C, const INDEXTYPE ldc);__global__ void gcfusedMM_bl1_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl1_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 1;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl1_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl2_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl2_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 2;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl2_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl3_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl3_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 3;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl3_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl4_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl4_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 4;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl4_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl5_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl5_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 5;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl5_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl6_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl6_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 6;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl6_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl7_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl7_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 7;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl7_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl8_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl8_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 8;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl8_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl9_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl9_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 9;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl9_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl10_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl10_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 10;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl10_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl11_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl11_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 11;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl11_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl12_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl12_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 12;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl12_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl13_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl13_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 13;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl13_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl14_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl14_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 14;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl14_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl15_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl15_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 15;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl15_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl16_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl16_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 16;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl16_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl17_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl17_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 17;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl17_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl18_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl18_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 18;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl18_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl19_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl19_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 19;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl19_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl20_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl20_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 20;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl20_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl21_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl21_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 21;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl21_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl22_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl22_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 22;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl22_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl23_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl23_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 23;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl23_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl24_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl24_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 24;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl24_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl25_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl25_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 25;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl25_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl26_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl26_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 26;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl26_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl27_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl27_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 27;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl27_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl28_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl28_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 28;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl28_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl29_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl29_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 29;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl29_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl30_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl30_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 30;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl30_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl31_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl31_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 31;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl31_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl32_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl32_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 32;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl32_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl33_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl33_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 33;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl33_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl34_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl34_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 34;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl34_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl35_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl35_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 35;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl35_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl36_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl36_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 36;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl36_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl37_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl37_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 37;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl37_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl38_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl38_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 38;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl38_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl39_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl39_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 39;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl39_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl40_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl40_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 40;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl40_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl41_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl41_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 41;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl41_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl42_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl42_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 42;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl42_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl43_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl43_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 43;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl43_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl44_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl44_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 44;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl44_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl45_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl45_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 45;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl45_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl46_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl46_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 46;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl46_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl47_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl47_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 47;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl47_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl48_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl48_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 48;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl48_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl49_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl49_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 49;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl49_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl50_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl50_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 50;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl50_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl51_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl51_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 51;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl51_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl52_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl52_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 52;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl52_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl53_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl53_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 53;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl53_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl54_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl54_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 54;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl54_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl55_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl55_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 55;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl55_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl56_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl56_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 56;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl56_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl57_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl57_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 57;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl57_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl58_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl58_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 58;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl58_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl59_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl59_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 59;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl59_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl60_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl60_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 60;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl60_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl61_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl61_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 61;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl61_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl62_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl62_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 62;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl62_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl63_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl63_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 63;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl63_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl64_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl64_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 64;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl64_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl65_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl65_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 65;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl65_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl66_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl66_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 66;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl66_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl67_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl67_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 67;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl67_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl68_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl68_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 68;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl68_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl69_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl69_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 69;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl69_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl70_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl70_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 70;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl70_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl71_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl71_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 71;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl71_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl72_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl72_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 72;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl72_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl73_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl73_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 73;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl73_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl74_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl74_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 74;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl74_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl75_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl75_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 75;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl75_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl76_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl76_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 76;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl76_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl77_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl77_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 77;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl77_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl78_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl78_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 78;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl78_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl79_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl79_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 79;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl79_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl80_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl80_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 80;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl80_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl81_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl81_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 81;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl81_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl82_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl82_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 82;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl82_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl83_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl83_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 83;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl83_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl84_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl84_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 84;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl84_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl85_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl85_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 85;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl85_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl86_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl86_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 86;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl86_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl87_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl87_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 87;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl87_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl88_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl88_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 88;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl88_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl89_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl89_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 89;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl89_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl90_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl90_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 90;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl90_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl91_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl91_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 91;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl91_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl92_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl92_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 92;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl92_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl93_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl93_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 93;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl93_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl94_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl94_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 94;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl94_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl95_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl95_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 95;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl95_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl96_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl96_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 96;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl96_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl97_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl97_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 97;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl97_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl98_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl98_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 98;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl98_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl99_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl99_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 99;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl99_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl100_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl100_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 100;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl100_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl101_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl101_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 101;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl101_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl102_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl102_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 102;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl102_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl103_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl103_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 103;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl103_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl104_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl104_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 104;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl104_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl105_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl105_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 105;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl105_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl106_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl106_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 106;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl106_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl107_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl107_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 107;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl107_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl108_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl108_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 108;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl108_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl109_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl109_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 109;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl109_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl110_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl110_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 110;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl110_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl111_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl111_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 111;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl111_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl112_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl112_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 112;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl112_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl113_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl113_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 113;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl113_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl114_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl114_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 114;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl114_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl115_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl115_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 115;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl115_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl116_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl116_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 116;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl116_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl117_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl117_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 117;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl117_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl118_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl118_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 118;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl118_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl119_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl119_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 119;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl119_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl120_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl120_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 120;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl120_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl121_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl121_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 121;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl121_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl122_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl122_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 122;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl122_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl123_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl123_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 123;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl123_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl124_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl124_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 124;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl124_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl125_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl125_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 125;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl125_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl126_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl126_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 126;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl126_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl127_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl127_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 127;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl127_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl128_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl128_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 128;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl128_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl129_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl129_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 129;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl129_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl130_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl130_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 130;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl130_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl131_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl131_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 131;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl131_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl132_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl132_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 132;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl132_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl133_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl133_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 133;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl133_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl134_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl134_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 134;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl134_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl135_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl135_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 135;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl135_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl136_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl136_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 136;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl136_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl137_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl137_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 137;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl137_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl138_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl138_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 138;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl138_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl139_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl139_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 139;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl139_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl140_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl140_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 140;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl140_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl141_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl141_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 141;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl141_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl142_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl142_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 142;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl142_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl143_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl143_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 143;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl143_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl144_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl144_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 144;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl144_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl145_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl145_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 145;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl145_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl146_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl146_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 146;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl146_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl147_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl147_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 147;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl147_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl148_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl148_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 148;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl148_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl149_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl149_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 149;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl149_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl150_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl150_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 150;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl150_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl151_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl151_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 151;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl151_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl152_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl152_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 152;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl152_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl153_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl153_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 153;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl153_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl154_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl154_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 154;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl154_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl155_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl155_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 155;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl155_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl156_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl156_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 156;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl156_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl157_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl157_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 157;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl157_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl158_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl158_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 158;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl158_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl159_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl159_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 159;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl159_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl160_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl160_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 160;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl160_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl161_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl161_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 161;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl161_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl162_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl162_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 162;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl162_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl163_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl163_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 163;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl163_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl164_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl164_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 164;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl164_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl165_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl165_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 165;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl165_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl166_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl166_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 166;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl166_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl167_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl167_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 167;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl167_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl168_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl168_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 168;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl168_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl169_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl169_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 169;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl169_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl170_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl170_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 170;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl170_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl171_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl171_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 171;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl171_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl172_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl172_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 172;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl172_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl173_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl173_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 173;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl173_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl174_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl174_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 174;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl174_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl175_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl175_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 175;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl175_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl176_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl176_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 176;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl176_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl177_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl177_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 177;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl177_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl178_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl178_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 178;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl178_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl179_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl179_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 179;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl179_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl180_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl180_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 180;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl180_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl181_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl181_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 181;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl181_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl182_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl182_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 182;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl182_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl183_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl183_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 183;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl183_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl184_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl184_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 184;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl184_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl185_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl185_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 185;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl185_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl186_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl186_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 186;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl186_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl187_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl187_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 187;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl187_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl188_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl188_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 188;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl188_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl189_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl189_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 189;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl189_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl190_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl190_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 190;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl190_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl191_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl191_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 191;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl191_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl192_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl192_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 192;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl192_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl193_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl193_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 193;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl193_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl194_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl194_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 194;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl194_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl195_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl195_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 195;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl195_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl196_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl196_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 196;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl196_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl197_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl197_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 197;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl197_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl198_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl198_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 198;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl198_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl199_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl199_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 199;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl199_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl200_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl200_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 200;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl200_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl201_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl201_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 201;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl201_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl202_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl202_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 202;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl202_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl203_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl203_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 203;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl203_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl204_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl204_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 204;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl204_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl205_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl205_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 205;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl205_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl206_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl206_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 206;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl206_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl207_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl207_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 207;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl207_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl208_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl208_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 208;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl208_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl209_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl209_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 209;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl209_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl210_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl210_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 210;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl210_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl211_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl211_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 211;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl211_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl212_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl212_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 212;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl212_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl213_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl213_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 213;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl213_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl214_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl214_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 214;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl214_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl215_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl215_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 215;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl215_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl216_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl216_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 216;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl216_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl217_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl217_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 217;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl217_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl218_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl218_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 218;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl218_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl219_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl219_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 219;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl219_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl220_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl220_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 220;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl220_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl221_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl221_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 221;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl221_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl222_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl222_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 222;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl222_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl223_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl223_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 223;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl223_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl224_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl224_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 224;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl224_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl225_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl225_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 225;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl225_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl226_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl226_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 226;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl226_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl227_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl227_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 227;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl227_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl228_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl228_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 228;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl228_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl229_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl229_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 229;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl229_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl230_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl230_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 230;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl230_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl231_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl231_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 231;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl231_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl232_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl232_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 232;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl232_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl233_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl233_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 233;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl233_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl234_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl234_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 234;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl234_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl235_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl235_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 235;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl235_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl236_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl236_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 236;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl236_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl237_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl237_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 237;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl237_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl238_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl238_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 238;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl238_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl239_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl239_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 239;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl239_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl240_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl240_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 240;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl240_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl241_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl241_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 241;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl241_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl242_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl242_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 242;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl242_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl243_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl243_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 243;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl243_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl244_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl244_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 244;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl244_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl245_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl245_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 245;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl245_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl246_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl246_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 246;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl246_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl247_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl247_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 247;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl247_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl248_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl248_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 248;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl248_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl249_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl249_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 249;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl249_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl250_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl250_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 250;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl250_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl251_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl251_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 251;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl251_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl252_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl252_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 252;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl252_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl253_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl253_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 253;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl253_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl254_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl254_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 254;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl254_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl255_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl255_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 255;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl255_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl256_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl256_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 256;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl256_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl257_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl257_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 257;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl257_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl258_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl258_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 258;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl258_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl259_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl259_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 259;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl259_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl260_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl260_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 260;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl260_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl261_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl261_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 261;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl261_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl262_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl262_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 262;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl262_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl263_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl263_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 263;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl263_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl264_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl264_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 264;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl264_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl265_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl265_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 265;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl265_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl266_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl266_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 266;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl266_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl267_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl267_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 267;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl267_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl268_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl268_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 268;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl268_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl269_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl269_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 269;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl269_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl270_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl270_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 270;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl270_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl271_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl271_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 271;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl271_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl272_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl272_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 272;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl272_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl273_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl273_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 273;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl273_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl274_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl274_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 274;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl274_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl275_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl275_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 275;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl275_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl276_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl276_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 276;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl276_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl277_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl277_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 277;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl277_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl278_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl278_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 278;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl278_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl279_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl279_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 279;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl279_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl280_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl280_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 280;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl280_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl281_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl281_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 281;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl281_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl282_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl282_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 282;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl282_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl283_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl283_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 283;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl283_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl284_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl284_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 284;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl284_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl285_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl285_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 285;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl285_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl286_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl286_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 286;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl286_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl287_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl287_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 287;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl287_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl288_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl288_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 288;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl288_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl289_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl289_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 289;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl289_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl290_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl290_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 290;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl290_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl291_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl291_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 291;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl291_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl292_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl292_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 292;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl292_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl293_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl293_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 293;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl293_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl294_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl294_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 294;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl294_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl295_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl295_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 295;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl295_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl296_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl296_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 296;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl296_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl297_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl297_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 297;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl297_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl298_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl298_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 298;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl298_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl299_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl299_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 299;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl299_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl300_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl300_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 300;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl300_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl301_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl301_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 301;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl301_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl302_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl302_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 302;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl302_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl303_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl303_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 303;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl303_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl304_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl304_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 304;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl304_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl305_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl305_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 305;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl305_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl306_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl306_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 306;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl306_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl307_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl307_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 307;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl307_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl308_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl308_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 308;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl308_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl309_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl309_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 309;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl309_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl310_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl310_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 310;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl310_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl311_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl311_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 311;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl311_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl312_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl312_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 312;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl312_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl313_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl313_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 313;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl313_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl314_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl314_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 314;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl314_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl315_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl315_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 315;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl315_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl316_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl316_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 316;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl316_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl317_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl317_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 317;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl317_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl318_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl318_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 318;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl318_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl319_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl319_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 319;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl319_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl320_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl320_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 320;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl320_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl321_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl321_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 321;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl321_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl322_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl322_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 322;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl322_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl323_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl323_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 323;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl323_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl324_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl324_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 324;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl324_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl325_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl325_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 325;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl325_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl326_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl326_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 326;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl326_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl327_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl327_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 327;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl327_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl328_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl328_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 328;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl328_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl329_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl329_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 329;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl329_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl330_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl330_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 330;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl330_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl331_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl331_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 331;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl331_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl332_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl332_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 332;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl332_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl333_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl333_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 333;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl333_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl334_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl334_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 334;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl334_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl335_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl335_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 335;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl335_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl336_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl336_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 336;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl336_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl337_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl337_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 337;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl337_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl338_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl338_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 338;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl338_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl339_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl339_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 339;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl339_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl340_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl340_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 340;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl340_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl341_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl341_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 341;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl341_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl342_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl342_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 342;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl342_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl343_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl343_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 343;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl343_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl344_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl344_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 344;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl344_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl345_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl345_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 345;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl345_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl346_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl346_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 346;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl346_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl347_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl347_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 347;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl347_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl348_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl348_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 348;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl348_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl349_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl349_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 349;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl349_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl350_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl350_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 350;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl350_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl351_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl351_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 351;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl351_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl352_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl352_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 352;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl352_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl353_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl353_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 353;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl353_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl354_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl354_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 354;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl354_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl355_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl355_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 355;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl355_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl356_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl356_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 356;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl356_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl357_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl357_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 357;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl357_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl358_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl358_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 358;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl358_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl359_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl359_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 359;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl359_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl360_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl360_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 360;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl360_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl361_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl361_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 361;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl361_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl362_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl362_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 362;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl362_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl363_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl363_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 363;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl363_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl364_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl364_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 364;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl364_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl365_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl365_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 365;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl365_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl366_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl366_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 366;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl366_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl367_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl367_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 367;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl367_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl368_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl368_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 368;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl368_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl369_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl369_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 369;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl369_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl370_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl370_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 370;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl370_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl371_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl371_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 371;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl371_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl372_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl372_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 372;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl372_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl373_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl373_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 373;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl373_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl374_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl374_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 374;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl374_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl375_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl375_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 375;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl375_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl376_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl376_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 376;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl376_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl377_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl377_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 377;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl377_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl378_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl378_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 378;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl378_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl379_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl379_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 379;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl379_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl380_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl380_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 380;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl380_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl381_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl381_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 381;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl381_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl382_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl382_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 382;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl382_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl383_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl383_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 383;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl383_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl384_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl384_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 384;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl384_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl385_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl385_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 385;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl385_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl386_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl386_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 386;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl386_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl387_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl387_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 387;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl387_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl388_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl388_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 388;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl388_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl389_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl389_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 389;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl389_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl390_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl390_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 390;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl390_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl391_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl391_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 391;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl391_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl392_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl392_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 392;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl392_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl393_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl393_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 393;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl393_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl394_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl394_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 394;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl394_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl395_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl395_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 395;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl395_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl396_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl396_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 396;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl396_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl397_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl397_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 397;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl397_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl398_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl398_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 398;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl398_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl399_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl399_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 399;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl399_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl400_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl400_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 400;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl400_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl401_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl401_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 401;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl401_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl402_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl402_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 402;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl402_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl403_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl403_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 403;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl403_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl404_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl404_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 404;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl404_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl405_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl405_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 405;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl405_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl406_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl406_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 406;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl406_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl407_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl407_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 407;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl407_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl408_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl408_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 408;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl408_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl409_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl409_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 409;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl409_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl410_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl410_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 410;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl410_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl411_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl411_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 411;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl411_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl412_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl412_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 412;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl412_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl413_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl413_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 413;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl413_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl414_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl414_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 414;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl414_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl415_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl415_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 415;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl415_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl416_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl416_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 416;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl416_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl417_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl417_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 417;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl417_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl418_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl418_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 418;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl418_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl419_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl419_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 419;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl419_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl420_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl420_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 420;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl420_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl421_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl421_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 421;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl421_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl422_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl422_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 422;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl422_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl423_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl423_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 423;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl423_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl424_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl424_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 424;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl424_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl425_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl425_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 425;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl425_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl426_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl426_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 426;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl426_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl427_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl427_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 427;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl427_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl428_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl428_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 428;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl428_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl429_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl429_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 429;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl429_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl430_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl430_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 430;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl430_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl431_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl431_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 431;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl431_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl432_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl432_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 432;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl432_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl433_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl433_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 433;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl433_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl434_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl434_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 434;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl434_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl435_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl435_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 435;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl435_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl436_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl436_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 436;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl436_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl437_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl437_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 437;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl437_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl438_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl438_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 438;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl438_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl439_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl439_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 439;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl439_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl440_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl440_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 440;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl440_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl441_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl441_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 441;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl441_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl442_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl442_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 442;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl442_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl443_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl443_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 443;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl443_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl444_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl444_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 444;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl444_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl445_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl445_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 445;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl445_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl446_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl446_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 446;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl446_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl447_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl447_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 447;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl447_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl448_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl448_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 448;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl448_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl449_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl449_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 449;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl449_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl450_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl450_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 450;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl450_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl451_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl451_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 451;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl451_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl452_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl452_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 452;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl452_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl453_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl453_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 453;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl453_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl454_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl454_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 454;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl454_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl455_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl455_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 455;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl455_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl456_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl456_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 456;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl456_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl457_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl457_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 457;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl457_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl458_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl458_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 458;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl458_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl459_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl459_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 459;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl459_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl460_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl460_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 460;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl460_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl461_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl461_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 461;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl461_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl462_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl462_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 462;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl462_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl463_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl463_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 463;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl463_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl464_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl464_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 464;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl464_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl465_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl465_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 465;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl465_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl466_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl466_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 466;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl466_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl467_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl467_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 467;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl467_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl468_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl468_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 468;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl468_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl469_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl469_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 469;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl469_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl470_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl470_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 470;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl470_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl471_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl471_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 471;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl471_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl472_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl472_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 472;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl472_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl473_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl473_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 473;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl473_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl474_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl474_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 474;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl474_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl475_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl475_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 475;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl475_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl476_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl476_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 476;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl476_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl477_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl477_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 477;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl477_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl478_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl478_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 478;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl478_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl479_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl479_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 479;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl479_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl480_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl480_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 480;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl480_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl481_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl481_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 481;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl481_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl482_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl482_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 482;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl482_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl483_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl483_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 483;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl483_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl484_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl484_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 484;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl484_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl485_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl485_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 485;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl485_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl486_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl486_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 486;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl486_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl487_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl487_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 487;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl487_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl488_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl488_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 488;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl488_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl489_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl489_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 489;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl489_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl490_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl490_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 490;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl490_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl491_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl491_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 491;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl491_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl492_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl492_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 492;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl492_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl493_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl493_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 493;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl493_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl494_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl494_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 494;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl494_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl495_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl495_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 495;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl495_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl496_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl496_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 496;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl496_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl497_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl497_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 497;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl497_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl498_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl498_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 498;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl498_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl499_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl499_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 499;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl499_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl500_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl500_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 500;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl500_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl501_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl501_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 501;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl501_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl502_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl502_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 502;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl502_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl503_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl503_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 503;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl503_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl504_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl504_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 504;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl504_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl505_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl505_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 505;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl505_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl506_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl506_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 506;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl506_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl507_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl507_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 507;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl507_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl508_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl508_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 508;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl508_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl509_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl509_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 509;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl509_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl510_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl510_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 510;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl510_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl511_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl511_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 511;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl511_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl512_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl512_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 512;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl512_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl513_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl513_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 513;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl513_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl514_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl514_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 514;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl514_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl515_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl515_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 515;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl515_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl516_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl516_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 516;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl516_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl517_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl517_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 517;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl517_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl518_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl518_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 518;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl518_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl519_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl519_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 519;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl519_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl520_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl520_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 520;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl520_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl521_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl521_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 521;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl521_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl522_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl522_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 522;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl522_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl523_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl523_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 523;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl523_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl524_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl524_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 524;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl524_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl525_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl525_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 525;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl525_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl526_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl526_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 526;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl526_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl527_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl527_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 527;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl527_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl528_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl528_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 528;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl528_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl529_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl529_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 529;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl529_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl530_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl530_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 530;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl530_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl531_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl531_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 531;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl531_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl532_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl532_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 532;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl532_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl533_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl533_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 533;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl533_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl534_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl534_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 534;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl534_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl535_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl535_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 535;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl535_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl536_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl536_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 536;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl536_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl537_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl537_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 537;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl537_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl538_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl538_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 538;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl538_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl539_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl539_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 539;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl539_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl540_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl540_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 540;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl540_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl541_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl541_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 541;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl541_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl542_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl542_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 542;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl542_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl543_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl543_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 543;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl543_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl544_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl544_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 544;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl544_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl545_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl545_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 545;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl545_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl546_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl546_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 546;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl546_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl547_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl547_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 547;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl547_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl548_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl548_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 548;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl548_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl549_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl549_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 549;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl549_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl550_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl550_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 550;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl550_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl551_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl551_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 551;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl551_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl552_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl552_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 552;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl552_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl553_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl553_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 553;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl553_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl554_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl554_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 554;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl554_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl555_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl555_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 555;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl555_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl556_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl556_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 556;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl556_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl557_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl557_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 557;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl557_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl558_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl558_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 558;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl558_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl559_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl559_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 559;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl559_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl560_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl560_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 560;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl560_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl561_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl561_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 561;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl561_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl562_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl562_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 562;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl562_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl563_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl563_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 563;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl563_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl564_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl564_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 564;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl564_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl565_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl565_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 565;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl565_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl566_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl566_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 566;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl566_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl567_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl567_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 567;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl567_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl568_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl568_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 568;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl568_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl569_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl569_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 569;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl569_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl570_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl570_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 570;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl570_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl571_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl571_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 571;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl571_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl572_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl572_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 572;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl572_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl573_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl573_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 573;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl573_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl574_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl574_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 574;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl574_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl575_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl575_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 575;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl575_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl576_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl576_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 576;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl576_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl577_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl577_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 577;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl577_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl578_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl578_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 578;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl578_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl579_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl579_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 579;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl579_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl580_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl580_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 580;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl580_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl581_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl581_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 581;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl581_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl582_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl582_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 582;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl582_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl583_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl583_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 583;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl583_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl584_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl584_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 584;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl584_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl585_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl585_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 585;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl585_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl586_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl586_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 586;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl586_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl587_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl587_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 587;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl587_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl588_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl588_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 588;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl588_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl589_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl589_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 589;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl589_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl590_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl590_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 590;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl590_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl591_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl591_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 591;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl591_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl592_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl592_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 592;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl592_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl593_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl593_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 593;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl593_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl594_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl594_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 594;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl594_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl595_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl595_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 595;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl595_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl596_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl596_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 596;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl596_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl597_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl597_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 597;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl597_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl598_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl598_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 598;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl598_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl599_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl599_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 599;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl599_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl600_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl600_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 600;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl600_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl601_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl601_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 601;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl601_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl602_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl602_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 602;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl602_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl603_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl603_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 603;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl603_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl604_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl604_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 604;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl604_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl605_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl605_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 605;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl605_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl606_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl606_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 606;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl606_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl607_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl607_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 607;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl607_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl608_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl608_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 608;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl608_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl609_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl609_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 609;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl609_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl610_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl610_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 610;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl610_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl611_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl611_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 611;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl611_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl612_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl612_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 612;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl612_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl613_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl613_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 613;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl613_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl614_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl614_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 614;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl614_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl615_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl615_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 615;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl615_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl616_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl616_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 616;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl616_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl617_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl617_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 617;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl617_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl618_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl618_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 618;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl618_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl619_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl619_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 619;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl619_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl620_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl620_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 620;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl620_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl621_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl621_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 621;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl621_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl622_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl622_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 622;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl622_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl623_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl623_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 623;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl623_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl624_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl624_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 624;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl624_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl625_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl625_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 625;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl625_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl626_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl626_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 626;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl626_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl627_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl627_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 627;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl627_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl628_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl628_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 628;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl628_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl629_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl629_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 629;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl629_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl630_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl630_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 630;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl630_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl631_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl631_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 631;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl631_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl632_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl632_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 632;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl632_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl633_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl633_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 633;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl633_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl634_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl634_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 634;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl634_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl635_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl635_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 635;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl635_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl636_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl636_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 636;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl636_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl637_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl637_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 637;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl637_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl638_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl638_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 638;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl638_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl639_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl639_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 639;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl639_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl640_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl640_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 640;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl640_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl641_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl641_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 641;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl641_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl642_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl642_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 642;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl642_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl643_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl643_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 643;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl643_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl644_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl644_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 644;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl644_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl645_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl645_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 645;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl645_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl646_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl646_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 646;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl646_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl647_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl647_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 647;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl647_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl648_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl648_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 648;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl648_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl649_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl649_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 649;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl649_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl650_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl650_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 650;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl650_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl651_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl651_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 651;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl651_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl652_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl652_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 652;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl652_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl653_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl653_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 653;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl653_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl654_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl654_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 654;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl654_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl655_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl655_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 655;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl655_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl656_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl656_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 656;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl656_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl657_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl657_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 657;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl657_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl658_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl658_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 658;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl658_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl659_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl659_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 659;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl659_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl660_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl660_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 660;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl660_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl661_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl661_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 661;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl661_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl662_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl662_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 662;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl662_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl663_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl663_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 663;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl663_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl664_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl664_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 664;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl664_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl665_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl665_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 665;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl665_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl666_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl666_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 666;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl666_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl667_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl667_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 667;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl667_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl668_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl668_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 668;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl668_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl669_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl669_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 669;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl669_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl670_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl670_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 670;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl670_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl671_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl671_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 671;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl671_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl672_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl672_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 672;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl672_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl673_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl673_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 673;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl673_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl674_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl674_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 674;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl674_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl675_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl675_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 675;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl675_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl676_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl676_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 676;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl676_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl677_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl677_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 677;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl677_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl678_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl678_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 678;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl678_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl679_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl679_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 679;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl679_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl680_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl680_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 680;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl680_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl681_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl681_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 681;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl681_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl682_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl682_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 682;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl682_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl683_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl683_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 683;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl683_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl684_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl684_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 684;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl684_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl685_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl685_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 685;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl685_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl686_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl686_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 686;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl686_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl687_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl687_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 687;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl687_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl688_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl688_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 688;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl688_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl689_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl689_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 689;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl689_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl690_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl690_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 690;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl690_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl691_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl691_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 691;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl691_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl692_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl692_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 692;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl692_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl693_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl693_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 693;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl693_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl694_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl694_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 694;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl694_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl695_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl695_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 695;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl695_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl696_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl696_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 696;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl696_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl697_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl697_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 697;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl697_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl698_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl698_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 698;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl698_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl699_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl699_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 699;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl699_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl700_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl700_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 700;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl700_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl701_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl701_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 701;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl701_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl702_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl702_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 702;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl702_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl703_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl703_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 703;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl703_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl704_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl704_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 704;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl704_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl705_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl705_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 705;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl705_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl706_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl706_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 706;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl706_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl707_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl707_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 707;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl707_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl708_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl708_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 708;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl708_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl709_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl709_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 709;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl709_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl710_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl710_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 710;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl710_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl711_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl711_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 711;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl711_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl712_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl712_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 712;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl712_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl713_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl713_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 713;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl713_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl714_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl714_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 714;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl714_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl715_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl715_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 715;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl715_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl716_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl716_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 716;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl716_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl717_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl717_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 717;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl717_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl718_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl718_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 718;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl718_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl719_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl719_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 719;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl719_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl720_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl720_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 720;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl720_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl721_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl721_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 721;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl721_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl722_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl722_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 722;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl722_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl723_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl723_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 723;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl723_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl724_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl724_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 724;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl724_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl725_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl725_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 725;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl725_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl726_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl726_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 726;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl726_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl727_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl727_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 727;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl727_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl728_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl728_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 728;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl728_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl729_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl729_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 729;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl729_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl730_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl730_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 730;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl730_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl731_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl731_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 731;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl731_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl732_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl732_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 732;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl732_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl733_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl733_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 733;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl733_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl734_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl734_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 734;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl734_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl735_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl735_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 735;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl735_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl736_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl736_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 736;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl736_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl737_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl737_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 737;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl737_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl738_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl738_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 738;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl738_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl739_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl739_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 739;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl739_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl740_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl740_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 740;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl740_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl741_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl741_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 741;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl741_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl742_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl742_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 742;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl742_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl743_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl743_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 743;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl743_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl744_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl744_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 744;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl744_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl745_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl745_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 745;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl745_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl746_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl746_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 746;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl746_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl747_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl747_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 747;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl747_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl748_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl748_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 748;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl748_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl749_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl749_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 749;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl749_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl750_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl750_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 750;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl750_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl751_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl751_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 751;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl751_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl752_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl752_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 752;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl752_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl753_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl753_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 753;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl753_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl754_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl754_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 754;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl754_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl755_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl755_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 755;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl755_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl756_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl756_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 756;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl756_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl757_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl757_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 757;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl757_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl758_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl758_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 758;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl758_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl759_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl759_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 759;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl759_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl760_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl760_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 760;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl760_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl761_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl761_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 761;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl761_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl762_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl762_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 762;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl762_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl763_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl763_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 763;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl763_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl764_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl764_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 764;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl764_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl765_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl765_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 765;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl765_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl766_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl766_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 766;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl766_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl767_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl767_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 767;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl767_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl768_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl768_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 768;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl768_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl769_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl769_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 769;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl769_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl770_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl770_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 770;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl770_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl771_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl771_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 771;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl771_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl772_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl772_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 772;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl772_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl773_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl773_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 773;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl773_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl774_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl774_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 774;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl774_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl775_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl775_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 775;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl775_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl776_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl776_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 776;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl776_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl777_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl777_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 777;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl777_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl778_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl778_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 778;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl778_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl779_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl779_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 779;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl779_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl780_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl780_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 780;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl780_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl781_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl781_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 781;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl781_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl782_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl782_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 782;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl782_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl783_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl783_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 783;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl783_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl784_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl784_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 784;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl784_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl785_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl785_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 785;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl785_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl786_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl786_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 786;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl786_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl787_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl787_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 787;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl787_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl788_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl788_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 788;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl788_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl789_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl789_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 789;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl789_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl790_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl790_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 790;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl790_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl791_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl791_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 791;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl791_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl792_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl792_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 792;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl792_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl793_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl793_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 793;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl793_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl794_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl794_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 794;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl794_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl795_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl795_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 795;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl795_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl796_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl796_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 796;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl796_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl797_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl797_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 797;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl797_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl798_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl798_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 798;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl798_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl799_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl799_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 799;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl799_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl800_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl800_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 800;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl800_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl801_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl801_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 801;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl801_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl802_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl802_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 802;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl802_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl803_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl803_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 803;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl803_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl804_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl804_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 804;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl804_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl805_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl805_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 805;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl805_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl806_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl806_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 806;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl806_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl807_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl807_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 807;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl807_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl808_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl808_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 808;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl808_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl809_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl809_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 809;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl809_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl810_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl810_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 810;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl810_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl811_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl811_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 811;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl811_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl812_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl812_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 812;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl812_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl813_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl813_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 813;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl813_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl814_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl814_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 814;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl814_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl815_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl815_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 815;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl815_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl816_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl816_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 816;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl816_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl817_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl817_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 817;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl817_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl818_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl818_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 818;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl818_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl819_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl819_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 819;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl819_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl820_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl820_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 820;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl820_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl821_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl821_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 821;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl821_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl822_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl822_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 822;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl822_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl823_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl823_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 823;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl823_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl824_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl824_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 824;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl824_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl825_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl825_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 825;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl825_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl826_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl826_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 826;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl826_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl827_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl827_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 827;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl827_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl828_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl828_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 828;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl828_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl829_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl829_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 829;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl829_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl830_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl830_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 830;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl830_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl831_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl831_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 831;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl831_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl832_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl832_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 832;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl832_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl833_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl833_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 833;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl833_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl834_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl834_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 834;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl834_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl835_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl835_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 835;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl835_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl836_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl836_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 836;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl836_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl837_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl837_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 837;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl837_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl838_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl838_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 838;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl838_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl839_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl839_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 839;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl839_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl840_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl840_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 840;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl840_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl841_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl841_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 841;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl841_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl842_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl842_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 842;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl842_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl843_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl843_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 843;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl843_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl844_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl844_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 844;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl844_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl845_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl845_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 845;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl845_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl846_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl846_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 846;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl846_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl847_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl847_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 847;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl847_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl848_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl848_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 848;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl848_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl849_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl849_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 849;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl849_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl850_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl850_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 850;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl850_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl851_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl851_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 851;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl851_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl852_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl852_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 852;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl852_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl853_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl853_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 853;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl853_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl854_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl854_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 854;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl854_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl855_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl855_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 855;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl855_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl856_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl856_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 856;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl856_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl857_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl857_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 857;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl857_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl858_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl858_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 858;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl858_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl859_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl859_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 859;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl859_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl860_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl860_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 860;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl860_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl861_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl861_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 861;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl861_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl862_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl862_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 862;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl862_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl863_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl863_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 863;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl863_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl864_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl864_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 864;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl864_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl865_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl865_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 865;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl865_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl866_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl866_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 866;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl866_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl867_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl867_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 867;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl867_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl868_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl868_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 868;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl868_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl869_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl869_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 869;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl869_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl870_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl870_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 870;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl870_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl871_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl871_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 871;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl871_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl872_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl872_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 872;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl872_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl873_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl873_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 873;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl873_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl874_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl874_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 874;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl874_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl875_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl875_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 875;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl875_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl876_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl876_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 876;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl876_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl877_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl877_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 877;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl877_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl878_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl878_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 878;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl878_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl879_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl879_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 879;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl879_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl880_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl880_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 880;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl880_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl881_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl881_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 881;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl881_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl882_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl882_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 882;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl882_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl883_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl883_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 883;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl883_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl884_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl884_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 884;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl884_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl885_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl885_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 885;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl885_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl886_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl886_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 886;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl886_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl887_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl887_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 887;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl887_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl888_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl888_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 888;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl888_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl889_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl889_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 889;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl889_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl890_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl890_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 890;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl890_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl891_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl891_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 891;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl891_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl892_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl892_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 892;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl892_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl893_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl893_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 893;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl893_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl894_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl894_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 894;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl894_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl895_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl895_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 895;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl895_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl896_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl896_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 896;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl896_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl897_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl897_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 897;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl897_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl898_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl898_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 898;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl898_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl899_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl899_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 899;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl899_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl900_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl900_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 900;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl900_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl901_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl901_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 901;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl901_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl902_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl902_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 902;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl902_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl903_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl903_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 903;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl903_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl904_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl904_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 904;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl904_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl905_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl905_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 905;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl905_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl906_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl906_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 906;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl906_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl907_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl907_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 907;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl907_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl908_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl908_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 908;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl908_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl909_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl909_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 909;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl909_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl910_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl910_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 910;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl910_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl911_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl911_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 911;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl911_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl912_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl912_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 912;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl912_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl913_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl913_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 913;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl913_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl914_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl914_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 914;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl914_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl915_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl915_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 915;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl915_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl916_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl916_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 916;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl916_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl917_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl917_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 917;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl917_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl918_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl918_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 918;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl918_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl919_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl919_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 919;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl919_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl920_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl920_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 920;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl920_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl921_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl921_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 921;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl921_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl922_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl922_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 922;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl922_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl923_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl923_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 923;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl923_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl924_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl924_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 924;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl924_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl925_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl925_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 925;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl925_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl926_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl926_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 926;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl926_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl927_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl927_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 927;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl927_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl928_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl928_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 928;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl928_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl929_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl929_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 929;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl929_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl930_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl930_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 930;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl930_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl931_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl931_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 931;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl931_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl932_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl932_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 932;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl932_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl933_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl933_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 933;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl933_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl934_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl934_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 934;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl934_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl935_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl935_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 935;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl935_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl936_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl936_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 936;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl936_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl937_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl937_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 937;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl937_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl938_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl938_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 938;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl938_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl939_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl939_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 939;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl939_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl940_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl940_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 940;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl940_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl941_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl941_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 941;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl941_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl942_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl942_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 942;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl942_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl943_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl943_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 943;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl943_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl944_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl944_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 944;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl944_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl945_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl945_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 945;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl945_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl946_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl946_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 946;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl946_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl947_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl947_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 947;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl947_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl948_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl948_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 948;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl948_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl949_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl949_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 949;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl949_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl950_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl950_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 950;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl950_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl951_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl951_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 951;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl951_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl952_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl952_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 952;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl952_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl953_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl953_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 953;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl953_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl954_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl954_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 954;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl954_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl955_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl955_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 955;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl955_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl956_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl956_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 956;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl956_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl957_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl957_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 957;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl957_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl958_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl958_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 958;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl958_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl959_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl959_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 959;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl959_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl960_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl960_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 960;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl960_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl961_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl961_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 961;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl961_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl962_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl962_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 962;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl962_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl963_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl963_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 963;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl963_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl964_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl964_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 964;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl964_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl965_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl965_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 965;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl965_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl966_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl966_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 966;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl966_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl967_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl967_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 967;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl967_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl968_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl968_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 968;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl968_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl969_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl969_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 969;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl969_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl970_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl970_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 970;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl970_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl971_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl971_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 971;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl971_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl972_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl972_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 972;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl972_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl973_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl973_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 973;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl973_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl974_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl974_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 974;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl974_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl975_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl975_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 975;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl975_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl976_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl976_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 976;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl976_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl977_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl977_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 977;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl977_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl978_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl978_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 978;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl978_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl979_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl979_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 979;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl979_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl980_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl980_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 980;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl980_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl981_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl981_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 981;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl981_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl982_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl982_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 982;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl982_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl983_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl983_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 983;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl983_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl984_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl984_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 984;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl984_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl985_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl985_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 985;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl985_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl986_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl986_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 986;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl986_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl987_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl987_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 987;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl987_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl988_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl988_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 988;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl988_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl989_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl989_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 989;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl989_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl990_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl990_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 990;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl990_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl991_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl991_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 991;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl991_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl992_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl992_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 992;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl992_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl993_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl993_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 993;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl993_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl994_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl994_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 994;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl994_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl995_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl995_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 995;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl995_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl996_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl996_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 996;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl996_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl997_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl997_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 997;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl997_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl998_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl998_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 998;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl998_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl999_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl999_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 999;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl999_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl1000_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl1000_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 1000;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl1000_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl1001_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl1001_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 1001;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl1001_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl1002_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl1002_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 1002;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl1002_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl1003_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl1003_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 1003;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl1003_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl1004_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl1004_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 1004;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl1004_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl1005_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl1005_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 1005;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl1005_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl1006_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl1006_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 1006;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl1006_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl1007_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl1007_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 1007;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl1007_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl1008_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl1008_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 1008;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl1008_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl1009_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl1009_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 1009;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl1009_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl1010_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl1010_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 1010;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl1010_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl1011_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl1011_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 1011;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl1011_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl1012_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl1012_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 1012;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl1012_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl1013_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl1013_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 1013;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl1013_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl1014_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl1014_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 1014;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl1014_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl1015_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl1015_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 1015;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl1015_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl1016_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl1016_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 1016;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl1016_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl1017_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl1017_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 1017;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl1017_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl1018_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl1018_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 1018;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl1018_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl1019_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl1019_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 1019;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl1019_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl1020_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl1020_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 1020;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl1020_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl1021_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl1021_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 1021;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl1021_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl1022_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl1022_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 1022;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl1022_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl1023_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl1023_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 1023;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl1023_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


__global__ void gcfusedMM_bl1024_spmm_csr (int m, int n, int k, int nnz, 
                               const int64_t* indx, const int64_t* ptrb, const float* val, 
                               const float* b, float* c,  int nnz_per_block)
{
     int nnz_start = blockIdx.x * nnz_per_block;
    int nnz_end = min(nnz_start + nnz_per_block, nnz);

    for (int i = nnz_start; i < nnz_end; i++) {
        
        int row = ptrb[i];
        int col = indx[i];
        float val_ij = val[i];

        float temp = 0.0f;
        for (int j = threadIdx.x; j < k; j += blockDim.x) {
	    temp += val_ij * b[col * k + j];
        }
        atomicAdd(&c[row * k + threadIdx.x], temp);
    }
}


void gfusedMM_bl1024_spmm_csr
(
   const char tkern,       // kernel variations
   const INDEXTYPE m,      // rows of A 
   const INDEXTYPE n,      // rows of B
   const INDEXTYPE k,      // dimension: col of A and B
   const VALUETYPE alpha,  // not used yet  
   const INDEXTYPE nnz,    // nonzeros  
   const INDEXTYPE rows,   // number of rows for sparse matrix 
   const INDEXTYPE cols,   // number of columns for sparse matrix 
   const VALUETYPE *val,   // NNZ value  
   const INDEXTYPE *indx,  // colids -> column indices 
   const INDEXTYPE *pntrb, // starting index for rowptr
   const INDEXTYPE *pntre, // ending index for rowptr
   const VALUETYPE *a,     // Dense A (X) matrix
   const INDEXTYPE lda,    // leading dimension of A (col size since A row-major)  
   const VALUETYPE *b,     // Dense B matrix
   const INDEXTYPE ldb,    // leading dimension of B (col size since B row-major)  
   const VALUETYPE beta,   // beta value 
   VALUETYPE *c,           // Dense matrix c
   const INDEXTYPE ldc     // leading dimension of c (col size since C row-major) 
)
{

     int nnz_per_block = 1024;
     int threads_per_block = 256;
     int num_blocks = (nnz + nnz_per_block - 1) / nnz_per_block;
     gcfusedMM_bl1024_spmm_csr<<<num_blocks, threads_per_block>>>(m, n, k, nnz, indx, pntrb, val, b, c, nnz_per_block);
     cudaDeviceSynchronize();

}


gfusedMM_spmm_t gfusedMM_spmm[] = { gfusedMM_bl1_spmm_csr,
gfusedMM_bl2_spmm_csr,
gfusedMM_bl3_spmm_csr,
gfusedMM_bl4_spmm_csr,
gfusedMM_bl5_spmm_csr,
gfusedMM_bl6_spmm_csr,
gfusedMM_bl7_spmm_csr,
gfusedMM_bl8_spmm_csr,
gfusedMM_bl9_spmm_csr,
gfusedMM_bl10_spmm_csr,
gfusedMM_bl11_spmm_csr,
gfusedMM_bl12_spmm_csr,
gfusedMM_bl13_spmm_csr,
gfusedMM_bl14_spmm_csr,
gfusedMM_bl15_spmm_csr,
gfusedMM_bl16_spmm_csr,
gfusedMM_bl17_spmm_csr,
gfusedMM_bl18_spmm_csr,
gfusedMM_bl19_spmm_csr,
gfusedMM_bl20_spmm_csr,
gfusedMM_bl21_spmm_csr,
gfusedMM_bl22_spmm_csr,
gfusedMM_bl23_spmm_csr,
gfusedMM_bl24_spmm_csr,
gfusedMM_bl25_spmm_csr,
gfusedMM_bl26_spmm_csr,
gfusedMM_bl27_spmm_csr,
gfusedMM_bl28_spmm_csr,
gfusedMM_bl29_spmm_csr,
gfusedMM_bl30_spmm_csr,
gfusedMM_bl31_spmm_csr,
gfusedMM_bl32_spmm_csr,
gfusedMM_bl33_spmm_csr,
gfusedMM_bl34_spmm_csr,
gfusedMM_bl35_spmm_csr,
gfusedMM_bl36_spmm_csr,
gfusedMM_bl37_spmm_csr,
gfusedMM_bl38_spmm_csr,
gfusedMM_bl39_spmm_csr,
gfusedMM_bl40_spmm_csr,
gfusedMM_bl41_spmm_csr,
gfusedMM_bl42_spmm_csr,
gfusedMM_bl43_spmm_csr,
gfusedMM_bl44_spmm_csr,
gfusedMM_bl45_spmm_csr,
gfusedMM_bl46_spmm_csr,
gfusedMM_bl47_spmm_csr,
gfusedMM_bl48_spmm_csr,
gfusedMM_bl49_spmm_csr,
gfusedMM_bl50_spmm_csr,
gfusedMM_bl51_spmm_csr,
gfusedMM_bl52_spmm_csr,
gfusedMM_bl53_spmm_csr,
gfusedMM_bl54_spmm_csr,
gfusedMM_bl55_spmm_csr,
gfusedMM_bl56_spmm_csr,
gfusedMM_bl57_spmm_csr,
gfusedMM_bl58_spmm_csr,
gfusedMM_bl59_spmm_csr,
gfusedMM_bl60_spmm_csr,
gfusedMM_bl61_spmm_csr,
gfusedMM_bl62_spmm_csr,
gfusedMM_bl63_spmm_csr,
gfusedMM_bl64_spmm_csr,
gfusedMM_bl65_spmm_csr,
gfusedMM_bl66_spmm_csr,
gfusedMM_bl67_spmm_csr,
gfusedMM_bl68_spmm_csr,
gfusedMM_bl69_spmm_csr,
gfusedMM_bl70_spmm_csr,
gfusedMM_bl71_spmm_csr,
gfusedMM_bl72_spmm_csr,
gfusedMM_bl73_spmm_csr,
gfusedMM_bl74_spmm_csr,
gfusedMM_bl75_spmm_csr,
gfusedMM_bl76_spmm_csr,
gfusedMM_bl77_spmm_csr,
gfusedMM_bl78_spmm_csr,
gfusedMM_bl79_spmm_csr,
gfusedMM_bl80_spmm_csr,
gfusedMM_bl81_spmm_csr,
gfusedMM_bl82_spmm_csr,
gfusedMM_bl83_spmm_csr,
gfusedMM_bl84_spmm_csr,
gfusedMM_bl85_spmm_csr,
gfusedMM_bl86_spmm_csr,
gfusedMM_bl87_spmm_csr,
gfusedMM_bl88_spmm_csr,
gfusedMM_bl89_spmm_csr,
gfusedMM_bl90_spmm_csr,
gfusedMM_bl91_spmm_csr,
gfusedMM_bl92_spmm_csr,
gfusedMM_bl93_spmm_csr,
gfusedMM_bl94_spmm_csr,
gfusedMM_bl95_spmm_csr,
gfusedMM_bl96_spmm_csr,
gfusedMM_bl97_spmm_csr,
gfusedMM_bl98_spmm_csr,
gfusedMM_bl99_spmm_csr,
gfusedMM_bl100_spmm_csr,
gfusedMM_bl101_spmm_csr,
gfusedMM_bl102_spmm_csr,
gfusedMM_bl103_spmm_csr,
gfusedMM_bl104_spmm_csr,
gfusedMM_bl105_spmm_csr,
gfusedMM_bl106_spmm_csr,
gfusedMM_bl107_spmm_csr,
gfusedMM_bl108_spmm_csr,
gfusedMM_bl109_spmm_csr,
gfusedMM_bl110_spmm_csr,
gfusedMM_bl111_spmm_csr,
gfusedMM_bl112_spmm_csr,
gfusedMM_bl113_spmm_csr,
gfusedMM_bl114_spmm_csr,
gfusedMM_bl115_spmm_csr,
gfusedMM_bl116_spmm_csr,
gfusedMM_bl117_spmm_csr,
gfusedMM_bl118_spmm_csr,
gfusedMM_bl119_spmm_csr,
gfusedMM_bl120_spmm_csr,
gfusedMM_bl121_spmm_csr,
gfusedMM_bl122_spmm_csr,
gfusedMM_bl123_spmm_csr,
gfusedMM_bl124_spmm_csr,
gfusedMM_bl125_spmm_csr,
gfusedMM_bl126_spmm_csr,
gfusedMM_bl127_spmm_csr,
gfusedMM_bl128_spmm_csr,
gfusedMM_bl129_spmm_csr,
gfusedMM_bl130_spmm_csr,
gfusedMM_bl131_spmm_csr,
gfusedMM_bl132_spmm_csr,
gfusedMM_bl133_spmm_csr,
gfusedMM_bl134_spmm_csr,
gfusedMM_bl135_spmm_csr,
gfusedMM_bl136_spmm_csr,
gfusedMM_bl137_spmm_csr,
gfusedMM_bl138_spmm_csr,
gfusedMM_bl139_spmm_csr,
gfusedMM_bl140_spmm_csr,
gfusedMM_bl141_spmm_csr,
gfusedMM_bl142_spmm_csr,
gfusedMM_bl143_spmm_csr,
gfusedMM_bl144_spmm_csr,
gfusedMM_bl145_spmm_csr,
gfusedMM_bl146_spmm_csr,
gfusedMM_bl147_spmm_csr,
gfusedMM_bl148_spmm_csr,
gfusedMM_bl149_spmm_csr,
gfusedMM_bl150_spmm_csr,
gfusedMM_bl151_spmm_csr,
gfusedMM_bl152_spmm_csr,
gfusedMM_bl153_spmm_csr,
gfusedMM_bl154_spmm_csr,
gfusedMM_bl155_spmm_csr,
gfusedMM_bl156_spmm_csr,
gfusedMM_bl157_spmm_csr,
gfusedMM_bl158_spmm_csr,
gfusedMM_bl159_spmm_csr,
gfusedMM_bl160_spmm_csr,
gfusedMM_bl161_spmm_csr,
gfusedMM_bl162_spmm_csr,
gfusedMM_bl163_spmm_csr,
gfusedMM_bl164_spmm_csr,
gfusedMM_bl165_spmm_csr,
gfusedMM_bl166_spmm_csr,
gfusedMM_bl167_spmm_csr,
gfusedMM_bl168_spmm_csr,
gfusedMM_bl169_spmm_csr,
gfusedMM_bl170_spmm_csr,
gfusedMM_bl171_spmm_csr,
gfusedMM_bl172_spmm_csr,
gfusedMM_bl173_spmm_csr,
gfusedMM_bl174_spmm_csr,
gfusedMM_bl175_spmm_csr,
gfusedMM_bl176_spmm_csr,
gfusedMM_bl177_spmm_csr,
gfusedMM_bl178_spmm_csr,
gfusedMM_bl179_spmm_csr,
gfusedMM_bl180_spmm_csr,
gfusedMM_bl181_spmm_csr,
gfusedMM_bl182_spmm_csr,
gfusedMM_bl183_spmm_csr,
gfusedMM_bl184_spmm_csr,
gfusedMM_bl185_spmm_csr,
gfusedMM_bl186_spmm_csr,
gfusedMM_bl187_spmm_csr,
gfusedMM_bl188_spmm_csr,
gfusedMM_bl189_spmm_csr,
gfusedMM_bl190_spmm_csr,
gfusedMM_bl191_spmm_csr,
gfusedMM_bl192_spmm_csr,
gfusedMM_bl193_spmm_csr,
gfusedMM_bl194_spmm_csr,
gfusedMM_bl195_spmm_csr,
gfusedMM_bl196_spmm_csr,
gfusedMM_bl197_spmm_csr,
gfusedMM_bl198_spmm_csr,
gfusedMM_bl199_spmm_csr,
gfusedMM_bl200_spmm_csr,
gfusedMM_bl201_spmm_csr,
gfusedMM_bl202_spmm_csr,
gfusedMM_bl203_spmm_csr,
gfusedMM_bl204_spmm_csr,
gfusedMM_bl205_spmm_csr,
gfusedMM_bl206_spmm_csr,
gfusedMM_bl207_spmm_csr,
gfusedMM_bl208_spmm_csr,
gfusedMM_bl209_spmm_csr,
gfusedMM_bl210_spmm_csr,
gfusedMM_bl211_spmm_csr,
gfusedMM_bl212_spmm_csr,
gfusedMM_bl213_spmm_csr,
gfusedMM_bl214_spmm_csr,
gfusedMM_bl215_spmm_csr,
gfusedMM_bl216_spmm_csr,
gfusedMM_bl217_spmm_csr,
gfusedMM_bl218_spmm_csr,
gfusedMM_bl219_spmm_csr,
gfusedMM_bl220_spmm_csr,
gfusedMM_bl221_spmm_csr,
gfusedMM_bl222_spmm_csr,
gfusedMM_bl223_spmm_csr,
gfusedMM_bl224_spmm_csr,
gfusedMM_bl225_spmm_csr,
gfusedMM_bl226_spmm_csr,
gfusedMM_bl227_spmm_csr,
gfusedMM_bl228_spmm_csr,
gfusedMM_bl229_spmm_csr,
gfusedMM_bl230_spmm_csr,
gfusedMM_bl231_spmm_csr,
gfusedMM_bl232_spmm_csr,
gfusedMM_bl233_spmm_csr,
gfusedMM_bl234_spmm_csr,
gfusedMM_bl235_spmm_csr,
gfusedMM_bl236_spmm_csr,
gfusedMM_bl237_spmm_csr,
gfusedMM_bl238_spmm_csr,
gfusedMM_bl239_spmm_csr,
gfusedMM_bl240_spmm_csr,
gfusedMM_bl241_spmm_csr,
gfusedMM_bl242_spmm_csr,
gfusedMM_bl243_spmm_csr,
gfusedMM_bl244_spmm_csr,
gfusedMM_bl245_spmm_csr,
gfusedMM_bl246_spmm_csr,
gfusedMM_bl247_spmm_csr,
gfusedMM_bl248_spmm_csr,
gfusedMM_bl249_spmm_csr,
gfusedMM_bl250_spmm_csr,
gfusedMM_bl251_spmm_csr,
gfusedMM_bl252_spmm_csr,
gfusedMM_bl253_spmm_csr,
gfusedMM_bl254_spmm_csr,
gfusedMM_bl255_spmm_csr,
gfusedMM_bl256_spmm_csr,
gfusedMM_bl257_spmm_csr,
gfusedMM_bl258_spmm_csr,
gfusedMM_bl259_spmm_csr,
gfusedMM_bl260_spmm_csr,
gfusedMM_bl261_spmm_csr,
gfusedMM_bl262_spmm_csr,
gfusedMM_bl263_spmm_csr,
gfusedMM_bl264_spmm_csr,
gfusedMM_bl265_spmm_csr,
gfusedMM_bl266_spmm_csr,
gfusedMM_bl267_spmm_csr,
gfusedMM_bl268_spmm_csr,
gfusedMM_bl269_spmm_csr,
gfusedMM_bl270_spmm_csr,
gfusedMM_bl271_spmm_csr,
gfusedMM_bl272_spmm_csr,
gfusedMM_bl273_spmm_csr,
gfusedMM_bl274_spmm_csr,
gfusedMM_bl275_spmm_csr,
gfusedMM_bl276_spmm_csr,
gfusedMM_bl277_spmm_csr,
gfusedMM_bl278_spmm_csr,
gfusedMM_bl279_spmm_csr,
gfusedMM_bl280_spmm_csr,
gfusedMM_bl281_spmm_csr,
gfusedMM_bl282_spmm_csr,
gfusedMM_bl283_spmm_csr,
gfusedMM_bl284_spmm_csr,
gfusedMM_bl285_spmm_csr,
gfusedMM_bl286_spmm_csr,
gfusedMM_bl287_spmm_csr,
gfusedMM_bl288_spmm_csr,
gfusedMM_bl289_spmm_csr,
gfusedMM_bl290_spmm_csr,
gfusedMM_bl291_spmm_csr,
gfusedMM_bl292_spmm_csr,
gfusedMM_bl293_spmm_csr,
gfusedMM_bl294_spmm_csr,
gfusedMM_bl295_spmm_csr,
gfusedMM_bl296_spmm_csr,
gfusedMM_bl297_spmm_csr,
gfusedMM_bl298_spmm_csr,
gfusedMM_bl299_spmm_csr,
gfusedMM_bl300_spmm_csr,
gfusedMM_bl301_spmm_csr,
gfusedMM_bl302_spmm_csr,
gfusedMM_bl303_spmm_csr,
gfusedMM_bl304_spmm_csr,
gfusedMM_bl305_spmm_csr,
gfusedMM_bl306_spmm_csr,
gfusedMM_bl307_spmm_csr,
gfusedMM_bl308_spmm_csr,
gfusedMM_bl309_spmm_csr,
gfusedMM_bl310_spmm_csr,
gfusedMM_bl311_spmm_csr,
gfusedMM_bl312_spmm_csr,
gfusedMM_bl313_spmm_csr,
gfusedMM_bl314_spmm_csr,
gfusedMM_bl315_spmm_csr,
gfusedMM_bl316_spmm_csr,
gfusedMM_bl317_spmm_csr,
gfusedMM_bl318_spmm_csr,
gfusedMM_bl319_spmm_csr,
gfusedMM_bl320_spmm_csr,
gfusedMM_bl321_spmm_csr,
gfusedMM_bl322_spmm_csr,
gfusedMM_bl323_spmm_csr,
gfusedMM_bl324_spmm_csr,
gfusedMM_bl325_spmm_csr,
gfusedMM_bl326_spmm_csr,
gfusedMM_bl327_spmm_csr,
gfusedMM_bl328_spmm_csr,
gfusedMM_bl329_spmm_csr,
gfusedMM_bl330_spmm_csr,
gfusedMM_bl331_spmm_csr,
gfusedMM_bl332_spmm_csr,
gfusedMM_bl333_spmm_csr,
gfusedMM_bl334_spmm_csr,
gfusedMM_bl335_spmm_csr,
gfusedMM_bl336_spmm_csr,
gfusedMM_bl337_spmm_csr,
gfusedMM_bl338_spmm_csr,
gfusedMM_bl339_spmm_csr,
gfusedMM_bl340_spmm_csr,
gfusedMM_bl341_spmm_csr,
gfusedMM_bl342_spmm_csr,
gfusedMM_bl343_spmm_csr,
gfusedMM_bl344_spmm_csr,
gfusedMM_bl345_spmm_csr,
gfusedMM_bl346_spmm_csr,
gfusedMM_bl347_spmm_csr,
gfusedMM_bl348_spmm_csr,
gfusedMM_bl349_spmm_csr,
gfusedMM_bl350_spmm_csr,
gfusedMM_bl351_spmm_csr,
gfusedMM_bl352_spmm_csr,
gfusedMM_bl353_spmm_csr,
gfusedMM_bl354_spmm_csr,
gfusedMM_bl355_spmm_csr,
gfusedMM_bl356_spmm_csr,
gfusedMM_bl357_spmm_csr,
gfusedMM_bl358_spmm_csr,
gfusedMM_bl359_spmm_csr,
gfusedMM_bl360_spmm_csr,
gfusedMM_bl361_spmm_csr,
gfusedMM_bl362_spmm_csr,
gfusedMM_bl363_spmm_csr,
gfusedMM_bl364_spmm_csr,
gfusedMM_bl365_spmm_csr,
gfusedMM_bl366_spmm_csr,
gfusedMM_bl367_spmm_csr,
gfusedMM_bl368_spmm_csr,
gfusedMM_bl369_spmm_csr,
gfusedMM_bl370_spmm_csr,
gfusedMM_bl371_spmm_csr,
gfusedMM_bl372_spmm_csr,
gfusedMM_bl373_spmm_csr,
gfusedMM_bl374_spmm_csr,
gfusedMM_bl375_spmm_csr,
gfusedMM_bl376_spmm_csr,
gfusedMM_bl377_spmm_csr,
gfusedMM_bl378_spmm_csr,
gfusedMM_bl379_spmm_csr,
gfusedMM_bl380_spmm_csr,
gfusedMM_bl381_spmm_csr,
gfusedMM_bl382_spmm_csr,
gfusedMM_bl383_spmm_csr,
gfusedMM_bl384_spmm_csr,
gfusedMM_bl385_spmm_csr,
gfusedMM_bl386_spmm_csr,
gfusedMM_bl387_spmm_csr,
gfusedMM_bl388_spmm_csr,
gfusedMM_bl389_spmm_csr,
gfusedMM_bl390_spmm_csr,
gfusedMM_bl391_spmm_csr,
gfusedMM_bl392_spmm_csr,
gfusedMM_bl393_spmm_csr,
gfusedMM_bl394_spmm_csr,
gfusedMM_bl395_spmm_csr,
gfusedMM_bl396_spmm_csr,
gfusedMM_bl397_spmm_csr,
gfusedMM_bl398_spmm_csr,
gfusedMM_bl399_spmm_csr,
gfusedMM_bl400_spmm_csr,
gfusedMM_bl401_spmm_csr,
gfusedMM_bl402_spmm_csr,
gfusedMM_bl403_spmm_csr,
gfusedMM_bl404_spmm_csr,
gfusedMM_bl405_spmm_csr,
gfusedMM_bl406_spmm_csr,
gfusedMM_bl407_spmm_csr,
gfusedMM_bl408_spmm_csr,
gfusedMM_bl409_spmm_csr,
gfusedMM_bl410_spmm_csr,
gfusedMM_bl411_spmm_csr,
gfusedMM_bl412_spmm_csr,
gfusedMM_bl413_spmm_csr,
gfusedMM_bl414_spmm_csr,
gfusedMM_bl415_spmm_csr,
gfusedMM_bl416_spmm_csr,
gfusedMM_bl417_spmm_csr,
gfusedMM_bl418_spmm_csr,
gfusedMM_bl419_spmm_csr,
gfusedMM_bl420_spmm_csr,
gfusedMM_bl421_spmm_csr,
gfusedMM_bl422_spmm_csr,
gfusedMM_bl423_spmm_csr,
gfusedMM_bl424_spmm_csr,
gfusedMM_bl425_spmm_csr,
gfusedMM_bl426_spmm_csr,
gfusedMM_bl427_spmm_csr,
gfusedMM_bl428_spmm_csr,
gfusedMM_bl429_spmm_csr,
gfusedMM_bl430_spmm_csr,
gfusedMM_bl431_spmm_csr,
gfusedMM_bl432_spmm_csr,
gfusedMM_bl433_spmm_csr,
gfusedMM_bl434_spmm_csr,
gfusedMM_bl435_spmm_csr,
gfusedMM_bl436_spmm_csr,
gfusedMM_bl437_spmm_csr,
gfusedMM_bl438_spmm_csr,
gfusedMM_bl439_spmm_csr,
gfusedMM_bl440_spmm_csr,
gfusedMM_bl441_spmm_csr,
gfusedMM_bl442_spmm_csr,
gfusedMM_bl443_spmm_csr,
gfusedMM_bl444_spmm_csr,
gfusedMM_bl445_spmm_csr,
gfusedMM_bl446_spmm_csr,
gfusedMM_bl447_spmm_csr,
gfusedMM_bl448_spmm_csr,
gfusedMM_bl449_spmm_csr,
gfusedMM_bl450_spmm_csr,
gfusedMM_bl451_spmm_csr,
gfusedMM_bl452_spmm_csr,
gfusedMM_bl453_spmm_csr,
gfusedMM_bl454_spmm_csr,
gfusedMM_bl455_spmm_csr,
gfusedMM_bl456_spmm_csr,
gfusedMM_bl457_spmm_csr,
gfusedMM_bl458_spmm_csr,
gfusedMM_bl459_spmm_csr,
gfusedMM_bl460_spmm_csr,
gfusedMM_bl461_spmm_csr,
gfusedMM_bl462_spmm_csr,
gfusedMM_bl463_spmm_csr,
gfusedMM_bl464_spmm_csr,
gfusedMM_bl465_spmm_csr,
gfusedMM_bl466_spmm_csr,
gfusedMM_bl467_spmm_csr,
gfusedMM_bl468_spmm_csr,
gfusedMM_bl469_spmm_csr,
gfusedMM_bl470_spmm_csr,
gfusedMM_bl471_spmm_csr,
gfusedMM_bl472_spmm_csr,
gfusedMM_bl473_spmm_csr,
gfusedMM_bl474_spmm_csr,
gfusedMM_bl475_spmm_csr,
gfusedMM_bl476_spmm_csr,
gfusedMM_bl477_spmm_csr,
gfusedMM_bl478_spmm_csr,
gfusedMM_bl479_spmm_csr,
gfusedMM_bl480_spmm_csr,
gfusedMM_bl481_spmm_csr,
gfusedMM_bl482_spmm_csr,
gfusedMM_bl483_spmm_csr,
gfusedMM_bl484_spmm_csr,
gfusedMM_bl485_spmm_csr,
gfusedMM_bl486_spmm_csr,
gfusedMM_bl487_spmm_csr,
gfusedMM_bl488_spmm_csr,
gfusedMM_bl489_spmm_csr,
gfusedMM_bl490_spmm_csr,
gfusedMM_bl491_spmm_csr,
gfusedMM_bl492_spmm_csr,
gfusedMM_bl493_spmm_csr,
gfusedMM_bl494_spmm_csr,
gfusedMM_bl495_spmm_csr,
gfusedMM_bl496_spmm_csr,
gfusedMM_bl497_spmm_csr,
gfusedMM_bl498_spmm_csr,
gfusedMM_bl499_spmm_csr,
gfusedMM_bl500_spmm_csr,
gfusedMM_bl501_spmm_csr,
gfusedMM_bl502_spmm_csr,
gfusedMM_bl503_spmm_csr,
gfusedMM_bl504_spmm_csr,
gfusedMM_bl505_spmm_csr,
gfusedMM_bl506_spmm_csr,
gfusedMM_bl507_spmm_csr,
gfusedMM_bl508_spmm_csr,
gfusedMM_bl509_spmm_csr,
gfusedMM_bl510_spmm_csr,
gfusedMM_bl511_spmm_csr,
gfusedMM_bl512_spmm_csr,
gfusedMM_bl513_spmm_csr,
gfusedMM_bl514_spmm_csr,
gfusedMM_bl515_spmm_csr,
gfusedMM_bl516_spmm_csr,
gfusedMM_bl517_spmm_csr,
gfusedMM_bl518_spmm_csr,
gfusedMM_bl519_spmm_csr,
gfusedMM_bl520_spmm_csr,
gfusedMM_bl521_spmm_csr,
gfusedMM_bl522_spmm_csr,
gfusedMM_bl523_spmm_csr,
gfusedMM_bl524_spmm_csr,
gfusedMM_bl525_spmm_csr,
gfusedMM_bl526_spmm_csr,
gfusedMM_bl527_spmm_csr,
gfusedMM_bl528_spmm_csr,
gfusedMM_bl529_spmm_csr,
gfusedMM_bl530_spmm_csr,
gfusedMM_bl531_spmm_csr,
gfusedMM_bl532_spmm_csr,
gfusedMM_bl533_spmm_csr,
gfusedMM_bl534_spmm_csr,
gfusedMM_bl535_spmm_csr,
gfusedMM_bl536_spmm_csr,
gfusedMM_bl537_spmm_csr,
gfusedMM_bl538_spmm_csr,
gfusedMM_bl539_spmm_csr,
gfusedMM_bl540_spmm_csr,
gfusedMM_bl541_spmm_csr,
gfusedMM_bl542_spmm_csr,
gfusedMM_bl543_spmm_csr,
gfusedMM_bl544_spmm_csr,
gfusedMM_bl545_spmm_csr,
gfusedMM_bl546_spmm_csr,
gfusedMM_bl547_spmm_csr,
gfusedMM_bl548_spmm_csr,
gfusedMM_bl549_spmm_csr,
gfusedMM_bl550_spmm_csr,
gfusedMM_bl551_spmm_csr,
gfusedMM_bl552_spmm_csr,
gfusedMM_bl553_spmm_csr,
gfusedMM_bl554_spmm_csr,
gfusedMM_bl555_spmm_csr,
gfusedMM_bl556_spmm_csr,
gfusedMM_bl557_spmm_csr,
gfusedMM_bl558_spmm_csr,
gfusedMM_bl559_spmm_csr,
gfusedMM_bl560_spmm_csr,
gfusedMM_bl561_spmm_csr,
gfusedMM_bl562_spmm_csr,
gfusedMM_bl563_spmm_csr,
gfusedMM_bl564_spmm_csr,
gfusedMM_bl565_spmm_csr,
gfusedMM_bl566_spmm_csr,
gfusedMM_bl567_spmm_csr,
gfusedMM_bl568_spmm_csr,
gfusedMM_bl569_spmm_csr,
gfusedMM_bl570_spmm_csr,
gfusedMM_bl571_spmm_csr,
gfusedMM_bl572_spmm_csr,
gfusedMM_bl573_spmm_csr,
gfusedMM_bl574_spmm_csr,
gfusedMM_bl575_spmm_csr,
gfusedMM_bl576_spmm_csr,
gfusedMM_bl577_spmm_csr,
gfusedMM_bl578_spmm_csr,
gfusedMM_bl579_spmm_csr,
gfusedMM_bl580_spmm_csr,
gfusedMM_bl581_spmm_csr,
gfusedMM_bl582_spmm_csr,
gfusedMM_bl583_spmm_csr,
gfusedMM_bl584_spmm_csr,
gfusedMM_bl585_spmm_csr,
gfusedMM_bl586_spmm_csr,
gfusedMM_bl587_spmm_csr,
gfusedMM_bl588_spmm_csr,
gfusedMM_bl589_spmm_csr,
gfusedMM_bl590_spmm_csr,
gfusedMM_bl591_spmm_csr,
gfusedMM_bl592_spmm_csr,
gfusedMM_bl593_spmm_csr,
gfusedMM_bl594_spmm_csr,
gfusedMM_bl595_spmm_csr,
gfusedMM_bl596_spmm_csr,
gfusedMM_bl597_spmm_csr,
gfusedMM_bl598_spmm_csr,
gfusedMM_bl599_spmm_csr,
gfusedMM_bl600_spmm_csr,
gfusedMM_bl601_spmm_csr,
gfusedMM_bl602_spmm_csr,
gfusedMM_bl603_spmm_csr,
gfusedMM_bl604_spmm_csr,
gfusedMM_bl605_spmm_csr,
gfusedMM_bl606_spmm_csr,
gfusedMM_bl607_spmm_csr,
gfusedMM_bl608_spmm_csr,
gfusedMM_bl609_spmm_csr,
gfusedMM_bl610_spmm_csr,
gfusedMM_bl611_spmm_csr,
gfusedMM_bl612_spmm_csr,
gfusedMM_bl613_spmm_csr,
gfusedMM_bl614_spmm_csr,
gfusedMM_bl615_spmm_csr,
gfusedMM_bl616_spmm_csr,
gfusedMM_bl617_spmm_csr,
gfusedMM_bl618_spmm_csr,
gfusedMM_bl619_spmm_csr,
gfusedMM_bl620_spmm_csr,
gfusedMM_bl621_spmm_csr,
gfusedMM_bl622_spmm_csr,
gfusedMM_bl623_spmm_csr,
gfusedMM_bl624_spmm_csr,
gfusedMM_bl625_spmm_csr,
gfusedMM_bl626_spmm_csr,
gfusedMM_bl627_spmm_csr,
gfusedMM_bl628_spmm_csr,
gfusedMM_bl629_spmm_csr,
gfusedMM_bl630_spmm_csr,
gfusedMM_bl631_spmm_csr,
gfusedMM_bl632_spmm_csr,
gfusedMM_bl633_spmm_csr,
gfusedMM_bl634_spmm_csr,
gfusedMM_bl635_spmm_csr,
gfusedMM_bl636_spmm_csr,
gfusedMM_bl637_spmm_csr,
gfusedMM_bl638_spmm_csr,
gfusedMM_bl639_spmm_csr,
gfusedMM_bl640_spmm_csr,
gfusedMM_bl641_spmm_csr,
gfusedMM_bl642_spmm_csr,
gfusedMM_bl643_spmm_csr,
gfusedMM_bl644_spmm_csr,
gfusedMM_bl645_spmm_csr,
gfusedMM_bl646_spmm_csr,
gfusedMM_bl647_spmm_csr,
gfusedMM_bl648_spmm_csr,
gfusedMM_bl649_spmm_csr,
gfusedMM_bl650_spmm_csr,
gfusedMM_bl651_spmm_csr,
gfusedMM_bl652_spmm_csr,
gfusedMM_bl653_spmm_csr,
gfusedMM_bl654_spmm_csr,
gfusedMM_bl655_spmm_csr,
gfusedMM_bl656_spmm_csr,
gfusedMM_bl657_spmm_csr,
gfusedMM_bl658_spmm_csr,
gfusedMM_bl659_spmm_csr,
gfusedMM_bl660_spmm_csr,
gfusedMM_bl661_spmm_csr,
gfusedMM_bl662_spmm_csr,
gfusedMM_bl663_spmm_csr,
gfusedMM_bl664_spmm_csr,
gfusedMM_bl665_spmm_csr,
gfusedMM_bl666_spmm_csr,
gfusedMM_bl667_spmm_csr,
gfusedMM_bl668_spmm_csr,
gfusedMM_bl669_spmm_csr,
gfusedMM_bl670_spmm_csr,
gfusedMM_bl671_spmm_csr,
gfusedMM_bl672_spmm_csr,
gfusedMM_bl673_spmm_csr,
gfusedMM_bl674_spmm_csr,
gfusedMM_bl675_spmm_csr,
gfusedMM_bl676_spmm_csr,
gfusedMM_bl677_spmm_csr,
gfusedMM_bl678_spmm_csr,
gfusedMM_bl679_spmm_csr,
gfusedMM_bl680_spmm_csr,
gfusedMM_bl681_spmm_csr,
gfusedMM_bl682_spmm_csr,
gfusedMM_bl683_spmm_csr,
gfusedMM_bl684_spmm_csr,
gfusedMM_bl685_spmm_csr,
gfusedMM_bl686_spmm_csr,
gfusedMM_bl687_spmm_csr,
gfusedMM_bl688_spmm_csr,
gfusedMM_bl689_spmm_csr,
gfusedMM_bl690_spmm_csr,
gfusedMM_bl691_spmm_csr,
gfusedMM_bl692_spmm_csr,
gfusedMM_bl693_spmm_csr,
gfusedMM_bl694_spmm_csr,
gfusedMM_bl695_spmm_csr,
gfusedMM_bl696_spmm_csr,
gfusedMM_bl697_spmm_csr,
gfusedMM_bl698_spmm_csr,
gfusedMM_bl699_spmm_csr,
gfusedMM_bl700_spmm_csr,
gfusedMM_bl701_spmm_csr,
gfusedMM_bl702_spmm_csr,
gfusedMM_bl703_spmm_csr,
gfusedMM_bl704_spmm_csr,
gfusedMM_bl705_spmm_csr,
gfusedMM_bl706_spmm_csr,
gfusedMM_bl707_spmm_csr,
gfusedMM_bl708_spmm_csr,
gfusedMM_bl709_spmm_csr,
gfusedMM_bl710_spmm_csr,
gfusedMM_bl711_spmm_csr,
gfusedMM_bl712_spmm_csr,
gfusedMM_bl713_spmm_csr,
gfusedMM_bl714_spmm_csr,
gfusedMM_bl715_spmm_csr,
gfusedMM_bl716_spmm_csr,
gfusedMM_bl717_spmm_csr,
gfusedMM_bl718_spmm_csr,
gfusedMM_bl719_spmm_csr,
gfusedMM_bl720_spmm_csr,
gfusedMM_bl721_spmm_csr,
gfusedMM_bl722_spmm_csr,
gfusedMM_bl723_spmm_csr,
gfusedMM_bl724_spmm_csr,
gfusedMM_bl725_spmm_csr,
gfusedMM_bl726_spmm_csr,
gfusedMM_bl727_spmm_csr,
gfusedMM_bl728_spmm_csr,
gfusedMM_bl729_spmm_csr,
gfusedMM_bl730_spmm_csr,
gfusedMM_bl731_spmm_csr,
gfusedMM_bl732_spmm_csr,
gfusedMM_bl733_spmm_csr,
gfusedMM_bl734_spmm_csr,
gfusedMM_bl735_spmm_csr,
gfusedMM_bl736_spmm_csr,
gfusedMM_bl737_spmm_csr,
gfusedMM_bl738_spmm_csr,
gfusedMM_bl739_spmm_csr,
gfusedMM_bl740_spmm_csr,
gfusedMM_bl741_spmm_csr,
gfusedMM_bl742_spmm_csr,
gfusedMM_bl743_spmm_csr,
gfusedMM_bl744_spmm_csr,
gfusedMM_bl745_spmm_csr,
gfusedMM_bl746_spmm_csr,
gfusedMM_bl747_spmm_csr,
gfusedMM_bl748_spmm_csr,
gfusedMM_bl749_spmm_csr,
gfusedMM_bl750_spmm_csr,
gfusedMM_bl751_spmm_csr,
gfusedMM_bl752_spmm_csr,
gfusedMM_bl753_spmm_csr,
gfusedMM_bl754_spmm_csr,
gfusedMM_bl755_spmm_csr,
gfusedMM_bl756_spmm_csr,
gfusedMM_bl757_spmm_csr,
gfusedMM_bl758_spmm_csr,
gfusedMM_bl759_spmm_csr,
gfusedMM_bl760_spmm_csr,
gfusedMM_bl761_spmm_csr,
gfusedMM_bl762_spmm_csr,
gfusedMM_bl763_spmm_csr,
gfusedMM_bl764_spmm_csr,
gfusedMM_bl765_spmm_csr,
gfusedMM_bl766_spmm_csr,
gfusedMM_bl767_spmm_csr,
gfusedMM_bl768_spmm_csr,
gfusedMM_bl769_spmm_csr,
gfusedMM_bl770_spmm_csr,
gfusedMM_bl771_spmm_csr,
gfusedMM_bl772_spmm_csr,
gfusedMM_bl773_spmm_csr,
gfusedMM_bl774_spmm_csr,
gfusedMM_bl775_spmm_csr,
gfusedMM_bl776_spmm_csr,
gfusedMM_bl777_spmm_csr,
gfusedMM_bl778_spmm_csr,
gfusedMM_bl779_spmm_csr,
gfusedMM_bl780_spmm_csr,
gfusedMM_bl781_spmm_csr,
gfusedMM_bl782_spmm_csr,
gfusedMM_bl783_spmm_csr,
gfusedMM_bl784_spmm_csr,
gfusedMM_bl785_spmm_csr,
gfusedMM_bl786_spmm_csr,
gfusedMM_bl787_spmm_csr,
gfusedMM_bl788_spmm_csr,
gfusedMM_bl789_spmm_csr,
gfusedMM_bl790_spmm_csr,
gfusedMM_bl791_spmm_csr,
gfusedMM_bl792_spmm_csr,
gfusedMM_bl793_spmm_csr,
gfusedMM_bl794_spmm_csr,
gfusedMM_bl795_spmm_csr,
gfusedMM_bl796_spmm_csr,
gfusedMM_bl797_spmm_csr,
gfusedMM_bl798_spmm_csr,
gfusedMM_bl799_spmm_csr,
gfusedMM_bl800_spmm_csr,
gfusedMM_bl801_spmm_csr,
gfusedMM_bl802_spmm_csr,
gfusedMM_bl803_spmm_csr,
gfusedMM_bl804_spmm_csr,
gfusedMM_bl805_spmm_csr,
gfusedMM_bl806_spmm_csr,
gfusedMM_bl807_spmm_csr,
gfusedMM_bl808_spmm_csr,
gfusedMM_bl809_spmm_csr,
gfusedMM_bl810_spmm_csr,
gfusedMM_bl811_spmm_csr,
gfusedMM_bl812_spmm_csr,
gfusedMM_bl813_spmm_csr,
gfusedMM_bl814_spmm_csr,
gfusedMM_bl815_spmm_csr,
gfusedMM_bl816_spmm_csr,
gfusedMM_bl817_spmm_csr,
gfusedMM_bl818_spmm_csr,
gfusedMM_bl819_spmm_csr,
gfusedMM_bl820_spmm_csr,
gfusedMM_bl821_spmm_csr,
gfusedMM_bl822_spmm_csr,
gfusedMM_bl823_spmm_csr,
gfusedMM_bl824_spmm_csr,
gfusedMM_bl825_spmm_csr,
gfusedMM_bl826_spmm_csr,
gfusedMM_bl827_spmm_csr,
gfusedMM_bl828_spmm_csr,
gfusedMM_bl829_spmm_csr,
gfusedMM_bl830_spmm_csr,
gfusedMM_bl831_spmm_csr,
gfusedMM_bl832_spmm_csr,
gfusedMM_bl833_spmm_csr,
gfusedMM_bl834_spmm_csr,
gfusedMM_bl835_spmm_csr,
gfusedMM_bl836_spmm_csr,
gfusedMM_bl837_spmm_csr,
gfusedMM_bl838_spmm_csr,
gfusedMM_bl839_spmm_csr,
gfusedMM_bl840_spmm_csr,
gfusedMM_bl841_spmm_csr,
gfusedMM_bl842_spmm_csr,
gfusedMM_bl843_spmm_csr,
gfusedMM_bl844_spmm_csr,
gfusedMM_bl845_spmm_csr,
gfusedMM_bl846_spmm_csr,
gfusedMM_bl847_spmm_csr,
gfusedMM_bl848_spmm_csr,
gfusedMM_bl849_spmm_csr,
gfusedMM_bl850_spmm_csr,
gfusedMM_bl851_spmm_csr,
gfusedMM_bl852_spmm_csr,
gfusedMM_bl853_spmm_csr,
gfusedMM_bl854_spmm_csr,
gfusedMM_bl855_spmm_csr,
gfusedMM_bl856_spmm_csr,
gfusedMM_bl857_spmm_csr,
gfusedMM_bl858_spmm_csr,
gfusedMM_bl859_spmm_csr,
gfusedMM_bl860_spmm_csr,
gfusedMM_bl861_spmm_csr,
gfusedMM_bl862_spmm_csr,
gfusedMM_bl863_spmm_csr,
gfusedMM_bl864_spmm_csr,
gfusedMM_bl865_spmm_csr,
gfusedMM_bl866_spmm_csr,
gfusedMM_bl867_spmm_csr,
gfusedMM_bl868_spmm_csr,
gfusedMM_bl869_spmm_csr,
gfusedMM_bl870_spmm_csr,
gfusedMM_bl871_spmm_csr,
gfusedMM_bl872_spmm_csr,
gfusedMM_bl873_spmm_csr,
gfusedMM_bl874_spmm_csr,
gfusedMM_bl875_spmm_csr,
gfusedMM_bl876_spmm_csr,
gfusedMM_bl877_spmm_csr,
gfusedMM_bl878_spmm_csr,
gfusedMM_bl879_spmm_csr,
gfusedMM_bl880_spmm_csr,
gfusedMM_bl881_spmm_csr,
gfusedMM_bl882_spmm_csr,
gfusedMM_bl883_spmm_csr,
gfusedMM_bl884_spmm_csr,
gfusedMM_bl885_spmm_csr,
gfusedMM_bl886_spmm_csr,
gfusedMM_bl887_spmm_csr,
gfusedMM_bl888_spmm_csr,
gfusedMM_bl889_spmm_csr,
gfusedMM_bl890_spmm_csr,
gfusedMM_bl891_spmm_csr,
gfusedMM_bl892_spmm_csr,
gfusedMM_bl893_spmm_csr,
gfusedMM_bl894_spmm_csr,
gfusedMM_bl895_spmm_csr,
gfusedMM_bl896_spmm_csr,
gfusedMM_bl897_spmm_csr,
gfusedMM_bl898_spmm_csr,
gfusedMM_bl899_spmm_csr,
gfusedMM_bl900_spmm_csr,
gfusedMM_bl901_spmm_csr,
gfusedMM_bl902_spmm_csr,
gfusedMM_bl903_spmm_csr,
gfusedMM_bl904_spmm_csr,
gfusedMM_bl905_spmm_csr,
gfusedMM_bl906_spmm_csr,
gfusedMM_bl907_spmm_csr,
gfusedMM_bl908_spmm_csr,
gfusedMM_bl909_spmm_csr,
gfusedMM_bl910_spmm_csr,
gfusedMM_bl911_spmm_csr,
gfusedMM_bl912_spmm_csr,
gfusedMM_bl913_spmm_csr,
gfusedMM_bl914_spmm_csr,
gfusedMM_bl915_spmm_csr,
gfusedMM_bl916_spmm_csr,
gfusedMM_bl917_spmm_csr,
gfusedMM_bl918_spmm_csr,
gfusedMM_bl919_spmm_csr,
gfusedMM_bl920_spmm_csr,
gfusedMM_bl921_spmm_csr,
gfusedMM_bl922_spmm_csr,
gfusedMM_bl923_spmm_csr,
gfusedMM_bl924_spmm_csr,
gfusedMM_bl925_spmm_csr,
gfusedMM_bl926_spmm_csr,
gfusedMM_bl927_spmm_csr,
gfusedMM_bl928_spmm_csr,
gfusedMM_bl929_spmm_csr,
gfusedMM_bl930_spmm_csr,
gfusedMM_bl931_spmm_csr,
gfusedMM_bl932_spmm_csr,
gfusedMM_bl933_spmm_csr,
gfusedMM_bl934_spmm_csr,
gfusedMM_bl935_spmm_csr,
gfusedMM_bl936_spmm_csr,
gfusedMM_bl937_spmm_csr,
gfusedMM_bl938_spmm_csr,
gfusedMM_bl939_spmm_csr,
gfusedMM_bl940_spmm_csr,
gfusedMM_bl941_spmm_csr,
gfusedMM_bl942_spmm_csr,
gfusedMM_bl943_spmm_csr,
gfusedMM_bl944_spmm_csr,
gfusedMM_bl945_spmm_csr,
gfusedMM_bl946_spmm_csr,
gfusedMM_bl947_spmm_csr,
gfusedMM_bl948_spmm_csr,
gfusedMM_bl949_spmm_csr,
gfusedMM_bl950_spmm_csr,
gfusedMM_bl951_spmm_csr,
gfusedMM_bl952_spmm_csr,
gfusedMM_bl953_spmm_csr,
gfusedMM_bl954_spmm_csr,
gfusedMM_bl955_spmm_csr,
gfusedMM_bl956_spmm_csr,
gfusedMM_bl957_spmm_csr,
gfusedMM_bl958_spmm_csr,
gfusedMM_bl959_spmm_csr,
gfusedMM_bl960_spmm_csr,
gfusedMM_bl961_spmm_csr,
gfusedMM_bl962_spmm_csr,
gfusedMM_bl963_spmm_csr,
gfusedMM_bl964_spmm_csr,
gfusedMM_bl965_spmm_csr,
gfusedMM_bl966_spmm_csr,
gfusedMM_bl967_spmm_csr,
gfusedMM_bl968_spmm_csr,
gfusedMM_bl969_spmm_csr,
gfusedMM_bl970_spmm_csr,
gfusedMM_bl971_spmm_csr,
gfusedMM_bl972_spmm_csr,
gfusedMM_bl973_spmm_csr,
gfusedMM_bl974_spmm_csr,
gfusedMM_bl975_spmm_csr,
gfusedMM_bl976_spmm_csr,
gfusedMM_bl977_spmm_csr,
gfusedMM_bl978_spmm_csr,
gfusedMM_bl979_spmm_csr,
gfusedMM_bl980_spmm_csr,
gfusedMM_bl981_spmm_csr,
gfusedMM_bl982_spmm_csr,
gfusedMM_bl983_spmm_csr,
gfusedMM_bl984_spmm_csr,
gfusedMM_bl985_spmm_csr,
gfusedMM_bl986_spmm_csr,
gfusedMM_bl987_spmm_csr,
gfusedMM_bl988_spmm_csr,
gfusedMM_bl989_spmm_csr,
gfusedMM_bl990_spmm_csr,
gfusedMM_bl991_spmm_csr,
gfusedMM_bl992_spmm_csr,
gfusedMM_bl993_spmm_csr,
gfusedMM_bl994_spmm_csr,
gfusedMM_bl995_spmm_csr,
gfusedMM_bl996_spmm_csr,
gfusedMM_bl997_spmm_csr,
gfusedMM_bl998_spmm_csr,
gfusedMM_bl999_spmm_csr,
gfusedMM_bl1000_spmm_csr,
gfusedMM_bl1001_spmm_csr,
gfusedMM_bl1002_spmm_csr,
gfusedMM_bl1003_spmm_csr,
gfusedMM_bl1004_spmm_csr,
gfusedMM_bl1005_spmm_csr,
gfusedMM_bl1006_spmm_csr,
gfusedMM_bl1007_spmm_csr,
gfusedMM_bl1008_spmm_csr,
gfusedMM_bl1009_spmm_csr,
gfusedMM_bl1010_spmm_csr,
gfusedMM_bl1011_spmm_csr,
gfusedMM_bl1012_spmm_csr,
gfusedMM_bl1013_spmm_csr,
gfusedMM_bl1014_spmm_csr,
gfusedMM_bl1015_spmm_csr,
gfusedMM_bl1016_spmm_csr,
gfusedMM_bl1017_spmm_csr,
gfusedMM_bl1018_spmm_csr,
gfusedMM_bl1019_spmm_csr,
gfusedMM_bl1020_spmm_csr,
gfusedMM_bl1021_spmm_csr,
gfusedMM_bl1022_spmm_csr,
gfusedMM_bl1023_spmm_csr,
gfusedMM_bl1024_spmm_csr };