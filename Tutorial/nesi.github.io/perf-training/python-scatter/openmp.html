<!DOCTYPE html>
<html lang="en">

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>OpenMP</title>
  <meta name="description" content="Objectives">

<!--  <link rel="stylesheet" type="text/css" href="/perf-training/python-scatter/openmp/assets/main.css" />  -->
<!--  <link rel="stylesheet" href="https://nesi.github.io/hpc_training/assets/main.css">  -->
<link rel="stylesheet" type="text/css" href="../assets/main.css" />


  <link rel="canonical" href="openmp.html">
  <link rel="alternate" type="application/rss+xml" title="New Zealand eScience Infrastructure - Performance Optimisation Training" href="../feed.xml">

    <!-- add border around tables -->
    <style>
        table {
            border-collapse: collapse;
            border-spacing: 0;
            border:2px solid #000000;
        }

        th {
            border:2px solid #000000;
        }

        td {
            border:1px solid #000000;
        }
    </style>

    <!-- previous/next links -->
    <style>
      .PageNavigation {
        font-size:14px;
        display: block;
        width: auto;
        overflow: hidden;
      }
      .PageNavigation a {
        display: block;
        width: 50%;
        float: left;
        margin: lem 0;
      }
      .PageNavigation .next {
        text-align: right;
        float: right;
      }
    </style>

  
  <!-- Start of nesi Zendesk Widget script -->
  <script async="async" id="ze-snippet" src="https://static.zdassets.com/ekr/snippet.js?key=27bb1239-2f9a-4a04-b53f-b92f22560adc"> </script>
  <!-- End of nesi Zendesk Widget script -->
</head>


  <body>

    <header class="site-header" role="banner">

  <div class="wrapper">
    
    

    <a href="https://www.nesi.org.nz/" class="pull-left">
    <img class="navbar-logo" src="../assets/img/NeSI_Logo_CMYK.jpg" alt="NeSI logo" />
          </a>

          <a class="site-title" href="../index.html">New Zealand eScience Infrastructure - Performance Optimisation Training</a>

    
      <nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
              <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
              <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger">
          
            
            
            <a class="page-link" href="../about/index.html">About</a>
            
          
            
            
            <a class="page-link" href="../python-scatter.html">Python Scatter</a>
            
          
        </div>
      </nav>
    
  </div>
</header>


    <main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title" itemprop="name headline">OpenMP</h1>
    <p class="post-meta">
      <time datetime="" itemprop="datePublished">
        
        
      </time>
      </p>
  </header>


  <div class="post-content" itemprop="articleBody">
    <ul>
  <li><a href="openmp.html#objectives">Objectives</a></li>
  <li><a href="openmp.html#why-implement-openmp-parallelisation">Why implement OpenMP parallelisation</a></li>
  <li><a href="openmp.html#what-is-openmp">What is OpenMP</a>
    <ul>
      <li><a href="openmp.html#pros">Pros</a></li>
      <li><a href="openmp.html#cons">Cons</a></li>
    </ul>
  </li>
  <li><a href="openmp.html#how-to-use-openmp">How to use OpenMP</a>
    <ul>
      <li><a href="openmp.html#compiler-switches">Compiler switches</a></li>
      <li><a href="openmp.html#directives">Directives</a></li>
      <li><a href="openmp.html#data-handling">Data handling</a></li>
      <li><a href="openmp.html#example">Example</a></li>
    </ul>
  </li>
  <li><a href="openmp.html#exercises">Exercises</a></li>
</ul>

    <h2 id="objectives">Objectives</h2>

<p>You will:</p>

<ul>
  <li>learn what threads are</li>
  <li>learn how to leverage OpenMP to accelerate your C/C++ code</li>
  <li>learn to compile C/C++ code with OpenMP enabled</li>
</ul>

<p>We will use the code in directory <code class="language-plaintext highlighter-rouge">openmp</code>. Start by</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>cd openmp
</code></pre></div></div>

<h2 id="why-implement-openmp-parallelisation">Why implement OpenMP parallelisation</h2>

<p>Most modern computers have multi-core CPUs. All cores of a CPU can access the same, shared memory. Furthermore, all the CPUs belonging to a node also share the same memory. Given that there are 36 cores and each core can execute two threads or work loads, up to 72 instructions can potentially be executed simulateneously with OpenMP on Mahuika.</p>

<h2 id="what-is-openmp">What is OpenMP</h2>

<p>OpenMP (Open Multi-Processing) is an application programming interface (API) for shared memory multiprocessing programming in C, C++ and Fortran.  An OpenMP-parallelised application starts as a serial application that runs on a single compute core. When instructed by the programmer, the application spawns a number of threads, which can run concurrently on separate cores. Thus, work can be distributed to leverage more resources.</p>

<p>Note that the OpenMP standard was recently extended to enable offloading computations to GPUs and other accelerators. However, not all compilers support this feature yet and there is a similar, competing standard called OpenACC that addresses the same use case. We will limit this lesson to multicore computing without offloading.</p>

<h3 id="pros">Pros</h3>

<ul>
  <li>supported by a large range of shared memory multicore architectures (virtually all modern CPUs have several cores) and accelerators</li>
  <li>the same source can be used to compile OpenMP and non-OpenMP code</li>
  <li>can be easier to implement than MPI parallelisation - a few lines of code may yield significant speedups</li>
  <li>can be more efficient than MPI as data in memory can be shared between multiple threads</li>
</ul>

<h3 id="cons">Cons</h3>

<ul>
  <li>limited to the resources of a single compute node</li>
  <li>it is possible to create “race conditions” where multiple threads overwrite each other’s work and such errors can be difficult to debug</li>
  <li>beware of external functions that are not “thread-safe” as these functions are susceptible to race conditions</li>
</ul>

<h2 id="how-to-use-openmp">How to use OpenMP</h2>

<p>To use OpenMP we need to</p>
<ul>
  <li>tell the compiler to spawn threads by adding directives to the code, specifying where and what to parallelise</li>
  <li>set the number of threads to be used at run time</li>
</ul>

<h3 id="compiler-switches">Compiler switches</h3>

<p>Use the following compiler switches:</p>
<ul>
  <li>GNU: <code class="language-plaintext highlighter-rouge">-fopenmp</code></li>
  <li>Intel: <code class="language-plaintext highlighter-rouge">-qopenmp</code></li>
  <li>Cray: OpenMP is enabled by default, use <code class="language-plaintext highlighter-rouge">-h noomp</code> to disable OpenMP</li>
</ul>

<h3 id="directives">Directives</h3>

<p>Parallelisation with OpenMP is implemented using directives, which are written as pragmas (C/C++) or specially formatted comments (Fortran). OpenMP also provides an additional Application Program Interface (API) that allows the program to configure and query the run time environment, e.g., to find out how many threads are running in parallel and which thread ID is running a given parallel section. For more information, have a look at the latest <a href="https://www.openmp.org/wp-content/uploads/openmp-4.5.pdf">OpenMP standard</a>.</p>

<p>OpenMP directives in the source code are interpreted by the compiler. The same source code can be used to build a serial or threaded version of the application by simply turning the OpenMP compiler switch on or off, and a non-OpenMP compiler will ignore the directives as unknown pragmas (C/C++) or as comments (Fortran).</p>

<p>OpenMP directives always start with:</p>
<ul>
  <li>C/C++: <code class="language-plaintext highlighter-rouge">#pragma omp</code></li>
  <li>Fortran (free form): <code class="language-plaintext highlighter-rouge">!$omp</code></li>
</ul>

<p>Directives are followed by the <em>directive names</em> and <em>clauses</em>, controlling parallelisation and data handling. OpenMP directives can consist of multiple statements and can be extended to multiple lines using line continuation characters such as <code class="language-plaintext highlighter-rouge">&amp;</code> (Fortran) or <code class="language-plaintext highlighter-rouge">\</code> (C/C++) at the end of the line.</p>

<p>There are various ways to distribute workloads for parallel execution, the most common being the parallel loop represented below:</p>

<p><a href="images/example_omp_threads.png"><img src="images/example_omp_threads.png" alt="example-mpi-gather" /></a></p>

<p>The application always starts in serial mode on a single thread (single arrow at the top). When requested, multiple threads are created/spawned (multiple arrows at the top). In this particular case, the number of threads is 3 (coloured boxes), and each thread performs three iterations (9 iterations altogether). Results are stored in separate elements of array <code class="language-plaintext highlighter-rouge">a</code> for each loop index <code class="language-plaintext highlighter-rouge">i</code>, so we do not create a race condition when the loop is executed in parallel. The program then resumes running on a single thread (single arrow at the bottom).</p>

<h3 id="data-handling">Data handling</h3>
<p>Because OpenMP is based on the shared memory programming model, most variables are shared by default. Other variables like loop index are meant to be private, i.e. the variable can take a different value for each thread. The programmer determines which variables are private and which are shared.</p>

<h3 id="example">Example</h3>
<p>As an example, we’ll assume that you have to compute the sum of the square of each element of an array:</p>
<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cm">/**
 * Compute the sum of the square of array elements
 * @param n number of elements
 * @param arr input array
 * @return res
 */</span>
<span class="k">extern</span> <span class="s">"C"</span>
<span class="kt">double</span> <span class="nf">mySumSq</span><span class="p">(</span><span class="kt">int</span> <span class="n">n</span><span class="p">,</span> <span class="kt">double</span><span class="o">*</span> <span class="n">arr</span><span class="p">)</span> <span class="p">{</span>
    <span class="kt">double</span> <span class="n">res</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
    <span class="cp">#pragma omp parallel for default(none) shared(arr,n) reduction(+:res)
</span>    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">n</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span> <span class="p">{</span>
        <span class="c1">// all variables defined inside the loop (including i) are private</span>
        <span class="n">res</span> <span class="o">+=</span> <span class="n">arr</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">arr</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
    <span class="p">}</span>
    <span class="k">return</span> <span class="n">res</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div>

<p>With the <code class="language-plaintext highlighter-rouge">parallel</code> statement we ask the compiler to spawn threads. The number of threads can be set <em>during run time</em> using the environment variable <code class="language-plaintext highlighter-rouge">OMP_NUM_THREADS</code>, which can be anything between 1 and the number of cores on a node, e.g., <code class="language-plaintext highlighter-rouge">export OMP_NUM_THREADS=8</code>.</p>

<p>Note, most applications do not scale to the maximum number of cores on a node due to non-uniform memory bandwidth, lack of load balancing between tasks and Amdahl’s law. The latter states that the maximum speedup is limited by the ratio of parallel to serial parts of the code. As a rule of thumb, if 5 percent of the time is spent in a part of the code that cannot be parallelised (that is a critical section or serial part) then the maximum parallel speedup is about 20 (or 1/0.05). There would be little point in using more than 20 threads in this case.</p>

<p>The <code class="language-plaintext highlighter-rouge">for</code> construct specifies that we want to parallelise the <code class="language-plaintext highlighter-rouge">for</code> loop that immediately follows the pragma. The different iterations of the loop will be then handled by different threads.</p>

<p>It is good practice to always use the <code class="language-plaintext highlighter-rouge">default(none)</code> clause, which forces us to declare the <code class="language-plaintext highlighter-rouge">shared</code> or <code class="language-plaintext highlighter-rouge">private</code> status of each variable defined <em>above</em> the parallel region. Variables that are defined <em>inside</em> the parallel region, such as loop index variable <code class="language-plaintext highlighter-rouge">i</code>, are automatically private.</p>

<p>It is generally good practice to define local variables inside the loop where possible. This will make your program easier to read and maintain and you won’t have to worry about creating race conditions by erroneously sharing a variable between threads. If you still need to declare, e.g., <code class="language-plaintext highlighter-rouge">myvariable</code> outside the loop, add the clause <code class="language-plaintext highlighter-rouge">private(myvariable)</code> to the OpenMP pragma.</p>

<p>Loop trip count <code class="language-plaintext highlighter-rouge">n</code> and data arrays <code class="language-plaintext highlighter-rouge">arr</code> can be shared as they are not changed inside the loop. Each thread will access the same data in memory, which is very efficient.</p>

<p>Variable <code class="language-plaintext highlighter-rouge">res</code> is special - it has to store the sum across all loop iterations at the end of the loop, even though individual iterations are executed by different threads. So <code class="language-plaintext highlighter-rouge">res</code> needs to be private to each thread at first and store partial sums. These partial sums then need to be collected by the original thread at the end of the loop to compute a grand total, which will be stored in <code class="language-plaintext highlighter-rouge">res</code> on that thread. The <code class="language-plaintext highlighter-rouge">reduction(+:res)</code> clause makes sure that the compiler will insert all required code to accomplish this.</p>

<blockquote>
  <h2 id="exercises">Exercises</h2>
  <p>The version in this directory has been partially parallelised.</p>
  <ul>
    <li>profile the execution for 8 threads and record its time</li>
    <li>add an OpenMP pragma at line indicated by <code class="language-plaintext highlighter-rouge">// ADD OPENMP PRAGMA HERE</code> in <code class="language-plaintext highlighter-rouge">src/wave.cpp</code> (assume function <code class="language-plaintext highlighter-rouge">computeScatteredWaveElement</code> to be thread-safe)</li>
    <li>re-run the code and report the new execution time</li>
  </ul>
</blockquote>

  </div>

  <hr />
  
  

  <div class="PageNavigation">
    
      <a class="prev" href="multiprocessing.html">&laquo; Multiprocessing</a>
    
    
      <a class="next" href="mpi.html">MPI parallelism &raquo;</a>
    
  </div>

  
</article>

      </div>
    </main>

    <footer class="site-footer">

  <div class="wrapper">

    <h2 class="footer-heading">New Zealand eScience Infrastructure - Performance Optimisation Training</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li>
            
              New Zealand eScience Infrastructure - Performance Optimisation Training
            
            </li>
            
            <li><a href="mailto:training@nesi.org.nz">training@nesi.org.nz</a></li>
            
        </ul>
      </div>

      <div class="footer-col footer-col-2">
        <ul class="social-media-list">
          

          
          <li>
            <a href="https://twitter.com/nesi_nz"><span class="icon icon--twitter"><svg viewBox="0 0 16 16" width="16px" height="16px"><path fill="#828282" d="M15.969,3.058c-0.586,0.26-1.217,0.436-1.878,0.515c0.675-0.405,1.194-1.045,1.438-1.809c-0.632,0.375-1.332,0.647-2.076,0.793c-0.596-0.636-1.446-1.033-2.387-1.033c-1.806,0-3.27,1.464-3.27,3.27 c0,0.256,0.029,0.506,0.085,0.745C5.163,5.404,2.753,4.102,1.14,2.124C0.859,2.607,0.698,3.168,0.698,3.767 c0,1.134,0.577,2.135,1.455,2.722C1.616,6.472,1.112,6.325,0.671,6.08c0,0.014,0,0.027,0,0.041c0,1.584,1.127,2.906,2.623,3.206 C3.02,9.402,2.731,9.442,2.433,9.442c-0.211,0-0.416-0.021-0.615-0.059c0.416,1.299,1.624,2.245,3.055,2.271 c-1.119,0.877-2.529,1.4-4.061,1.4c-0.264,0-0.524-0.015-0.78-0.046c1.447,0.928,3.166,1.469,5.013,1.469 c6.015,0,9.304-4.983,9.304-9.304c0-0.142-0.003-0.283-0.009-0.423C14.976,4.29,15.531,3.714,15.969,3.058z"/></svg>
</span><span class="username">nesi_nz</span></a>

          </li>
          
        </ul>
      </div>

      <div class="footer-col footer-col-3">
        <p>Training materials for a hands-on workshop on Performance Optimisation
</p>
      </div>
    </div>

    <div class="footer-col"> 

      <img class="navbar-logo" src="../assets/img/NeSI_Icon_Brand_expression_H_RGB.png" alt="NeSI logo" />
    </div>

  </div>

</footer>


  </body>

</html>
