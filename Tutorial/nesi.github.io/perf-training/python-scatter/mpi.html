<!DOCTYPE html>
<html lang="en">

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>MPI parallelism</title>
  <meta name="description" content="Objectives">

<!--  <link rel="stylesheet" type="text/css" href="/perf-training/python-scatter/mpi/assets/main.css" />  -->
<!--  <link rel="stylesheet" href="https://nesi.github.io/hpc_training/assets/main.css">  -->
<link rel="stylesheet" type="text/css" href="../assets/main.css" />


  <link rel="canonical" href="mpi.html">
  <link rel="alternate" type="application/rss+xml" title="New Zealand eScience Infrastructure - Performance Optimisation Training" href="../feed.xml">

    <!-- add border around tables -->
    <style>
        table {
            border-collapse: collapse;
            border-spacing: 0;
            border:2px solid #000000;
        }

        th {
            border:2px solid #000000;
        }

        td {
            border:1px solid #000000;
        }
    </style>

    <!-- previous/next links -->
    <style>
      .PageNavigation {
        font-size:14px;
        display: block;
        width: auto;
        overflow: hidden;
      }
      .PageNavigation a {
        display: block;
        width: 50%;
        float: left;
        margin: lem 0;
      }
      .PageNavigation .next {
        text-align: right;
        float: right;
      }
    </style>

  
  <!-- Start of nesi Zendesk Widget script -->
  <script async="async" id="ze-snippet" src="https://static.zdassets.com/ekr/snippet.js?key=27bb1239-2f9a-4a04-b53f-b92f22560adc"> </script>
  <!-- End of nesi Zendesk Widget script -->
</head>


  <body>

    <header class="site-header" role="banner">

  <div class="wrapper">
    
    

    <a href="https://www.nesi.org.nz/" class="pull-left">
    <img class="navbar-logo" src="../assets/img/NeSI_Logo_CMYK.jpg" alt="NeSI logo" />
          </a>

          <a class="site-title" href="../index.html">New Zealand eScience Infrastructure - Performance Optimisation Training</a>

    
      <nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
              <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
              <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger">
          
            
            
            <a class="page-link" href="../about/index.html">About</a>
            
          
            
            
            <a class="page-link" href="../python-scatter.html">Python Scatter</a>
            
          
        </div>
      </nav>
    
  </div>
</header>


    <main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title" itemprop="name headline">MPI parallelism</h1>
    <p class="post-meta">
      <time datetime="" itemprop="datePublished">
        
        
      </time>
      </p>
  </header>


  <div class="post-content" itemprop="articleBody">
    <ul>
  <li><a href="mpi.html#objectives">Objectives</a></li>
  <li><a href="mpi.html#what-is-mpi">What is MPI</a>
    <ul>
      <li><a href="mpi.html#pros">Pros</a></li>
      <li><a href="mpi.html#cons">Cons</a></li>
    </ul>
  </li>
  <li><a href="mpi.html#an-example-of-mpi-work-load-distribution">An example of MPI work load distribution</a></li>
  <li><a href="mpi.html#running-the-scatter-code-using-multiple-mpi-processes">Running the scatter code using multiple MPI processes</a>
    <ul>
      <li><a href="mpi.html#on-mahuika">On Mahuika</a></li>
      <li><a href="mpi.html#interactive-parallel-execution">Interactive parallel execution</a></li>
    </ul>
  </li>
  <li><a href="mpi.html#how-to-use-mpi-to-accelerate-scatterpy">How to use MPI to accelerate scatter.py</a></li>
  <li><a href="mpi.html#exercises">Exercises</a></li>
</ul>

    <h2 id="objectives">Objectives</h2>

<p>You will:</p>

<ul>
  <li>learn what MPI is</li>
  <li>how to parallelise code, and in particular:
    <ul>
      <li>learn how to disribute the work load among processes</li>
      <li>how to gather the results computed on separate processes</li>
    </ul>
  </li>
</ul>

<h2 id="what-is-mpi">What is MPI</h2>

<p>MPI is a standard application programming interface for creating and executing parallel programs. MPI was originally written for C, C++ and Fortran code but implementations have since become available for a variety of other languages, including Python.</p>

<p>MPI programs are started as a bunch of instances (often called “processes” or “tasks”) of an executable, which run concurrently.</p>

<p>As each process runs, the program may need to exchange data (“messages” - hence the name) with other processes. An example of data exchanges is point-to-point communication where one process sends data to another process. In other cases data may be “gathered” from multiple processes at once and sent to a root process. Inversely, data can be “scattered” from the root process to multiple other processes in one step.</p>

<h3 id="pros">Pros</h3>

<ul>
  <li>suitable for anything from laptops to supercomputers with hundreds of thousands of cores</li>
  <li>unlike shared memory approaches like OpenMP, MPI code can utilise resources beyond a single node</li>
  <li>can be used in combination with OpenMP and other parallelisation methods</li>
</ul>

<h3 id="cons">Cons</h3>

<ul>
  <li>there are no serial sections in MPI code and hence MPI programs need to be written to run in parallel from the beginning to the end</li>
  <li>some algorithms are difficult to implement efficiently with MPI due its distributed memory approach and communication overhead</li>
  <li>it is easy to create “dead locks” where one or more processes get stuck waiting for a message</li>
  <li>the requested computing resources are occupied the whole run time, i.e. there is no dynamic allocation/deallocation of CPU cores</li>
</ul>

<h2 id="an-example-of-mpi-work-load-distribution">An example of MPI work load distribution</h2>

<p>Let’s revisit our multiprocessing problem where we fill in an array by calling an expensive function</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">time</span>

<span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
	<span class="c1"># expensive function
</span>	<span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
	<span class="k">return</span> <span class="n">x</span>

<span class="c1"># call the function sequentially for input values 0, 1, 2, 3 and 4
</span><span class="n">input_values</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">)]</span>
<span class="n">res</span> <span class="o">=</span> <span class="p">[</span><span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">input_values</span><span class="p">]</span>
</code></pre></div></div>

<p>Further let’s assume that we have 3 processes to compute the 5 elements of our problem. To distribute the work load, each process will compute a few elements of the array. Process 0 will compute the first two elements, process 1 the next two elements, etc. until the last process which will be computing the remaining element. Once the elements of the array are computed, a gather operation collects the results into a single array on the last process. This is shown below with process 0 in red, process 1 in blue and process 2 in yellow.</p>

<p><a href="images/example-mpi-gather.png"><img src="images/example-mpi-gather.png" alt="example-mpi-gather" /></a></p>

<p>Neglecting the time it takes to gather the results, we can expect the execution time to be reduced from 5 time units (number of elements) to 2 time units (maximum number of elements handled by a process). Thus we get a speedup of 5/2 = 2.5x in this case. The ideal speedup is 3x but this cannot happen because process 2 has to wait until processes 0 and 1 finish. This is known as a <em>load balancing</em> problem, processes may take different amounts of time to a complete a task, causing some processes to stall (dashed line). Naturally, we should always strive to assign the same amount of work to each process.</p>

<p>Transferring data from processes 0-1 to 2 takes additional time. Hence 2.5x would be the maximum, achievable speedup for this case assuming that the work to compute each element is the same.</p>

<p>We will now implement the example using the <code class="language-plaintext highlighter-rouge">mpi4py</code> package in Python.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">time</span>
<span class="kn">from</span> <span class="nn">mpi4py</span> <span class="kn">import</span> <span class="n">MPI</span>
<span class="kn">import</span> <span class="nn">numpy</span>

<span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="c1"># expensive function
</span>    <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x</span>

<span class="c1"># default communicator - grab all processes
</span><span class="n">comm</span> <span class="o">=</span> <span class="n">MPI</span><span class="o">.</span><span class="n">COMM_WORLD</span>

<span class="c1"># number of processes
</span><span class="n">nprocs</span> <span class="o">=</span> <span class="n">comm</span><span class="o">.</span><span class="n">Get_size</span><span class="p">()</span>

<span class="c1"># identity of this process (process element, sometimes called rank)
</span><span class="n">pe</span> <span class="o">=</span> <span class="n">comm</span><span class="o">.</span><span class="n">Get_rank</span><span class="p">()</span>

<span class="c1"># special process responsible for administrative work
</span><span class="n">root</span> <span class="o">=</span> <span class="n">nprocs</span> <span class="o">-</span> <span class="mi">1</span>

<span class="c1"># total number of (work) elements
</span><span class="n">n_global</span> <span class="o">=</span> <span class="mi">5</span>

<span class="c1"># get the list of indices local to this process
</span><span class="n">local_inds</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array_split</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_global</span><span class="p">),</span> <span class="n">nprocs</span><span class="p">)[</span><span class="n">pe</span><span class="p">]</span>

<span class="c1"># allocate and set local input values 
</span><span class="n">local_input_values</span> <span class="o">=</span> <span class="n">local_inds</span>

<span class="n">local_res</span> <span class="o">=</span> <span class="p">[</span><span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">local_input_values</span><span class="p">]</span>

<span class="c1"># gather all local arrays on process root, will 
# return a list of numpy arrays
</span><span class="n">res_list</span> <span class="o">=</span> <span class="n">comm</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">local_res</span><span class="p">,</span> <span class="n">root</span><span class="o">=</span><span class="n">root</span><span class="p">)</span>

<span class="k">if</span> <span class="n">pe</span> <span class="o">==</span> <span class="n">root</span><span class="p">:</span>
    <span class="c1"># turn the list of arrays into a single array
</span>    <span class="n">res</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span><span class="n">res_list</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="n">res</span><span class="p">)</span>
</code></pre></div></div>

<p>We first need to get a “communicator” - this is our communication interface that allows us to exchange data with other MPI processes. <code class="language-plaintext highlighter-rouge">COMM_WORLD</code> is the default communicator that includes all the processes we launch, and we will use that in this simple example.</p>

<p>We can now ask our communicator how many MPI processes are running using the <code class="language-plaintext highlighter-rouge">Get_size()</code> method. The <code class="language-plaintext highlighter-rouge">Get_rank()</code> method returns the rank (a process number between 0 and <code class="language-plaintext highlighter-rouge">nprocs</code>) and stores the value in <code class="language-plaintext highlighter-rouge">pe</code>. Keep in mind that <code class="language-plaintext highlighter-rouge">pe</code> is the only differentiating factor between this and other MPI processes. Our program then needs to make decisions based on <code class="language-plaintext highlighter-rouge">pe</code>, to decide for instance on which subset of the data to operate on.</p>

<p>The process with rank <code class="language-plaintext highlighter-rouge">nprocs - 1</code> is earmarked here as “root”. The root process often does administrative work, such as gathering results from other processes, as shown in the diagram above. We are free to choose any MPI rank as root.</p>

<p>Now each process works on its own local array <code class="language-plaintext highlighter-rouge">local_input_values</code>, which is smaller than the actual array as it contains only the data assigned to a given process. In the above, array <code class="language-plaintext highlighter-rouge">local_input_values</code> is the set of indices from <code class="language-plaintext highlighter-rouge">numpy.arange(0, n_global)</code> which are local to that particular process. Each process will get a different  <code class="language-plaintext highlighter-rouge">local_input_values</code>. For good load balancing, we like <code class="language-plaintext highlighter-rouge">local_input_values</code> to have the same size across all processes and so we use <code class="language-plaintext highlighter-rouge">numpy.array_split</code> to decompose the array in an “optimal” way.</p>

<p>The results of the local calculations are stored in <code class="language-plaintext highlighter-rouge">local_res</code>. To gather all results on <code class="language-plaintext highlighter-rouge">root</code>, we now call MPI’s <code class="language-plaintext highlighter-rouge">gather</code> method on every process, hand over the different contributions, and tell MPI which rank we have chosen as root.</p>

<p>The last thing to do is to look at the results. It is important to realise that variable <code class="language-plaintext highlighter-rouge">res_list</code> only contains data on our <code class="language-plaintext highlighter-rouge">root</code> rank and nowhere else (it will be set to <code class="language-plaintext highlighter-rouge">None</code> on all other processes). We therefore add an <code class="language-plaintext highlighter-rouge">if</code> statement to make sure that <code class="language-plaintext highlighter-rouge">concatenate</code> and <code class="language-plaintext highlighter-rouge">print</code> are only executed on our root process.</p>

<h2 id="running-the-scatter-code-using-multiple-mpi-processes">Running the scatter code using multiple MPI processes</h2>

<p>We’ll use the code in directory <code class="language-plaintext highlighter-rouge">mpi</code>. Start by</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>cd mpi
</code></pre></div></div>

<h3 id="on-mahuika">On Mahuika</h3>

<p>To run using 8 processes type</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>srun --ntasks=8 python scatter.py
</code></pre></div></div>
<p>(with additional <code class="language-plaintext highlighter-rouge">srun</code> options such as <code class="language-plaintext highlighter-rouge">--account=</code> required).</p>

<h3 id="interactive-parallel-execution">Interactive parallel execution</h3>

<p>To run interactively using 8 processes, type</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>mpiexec -n 8 python scatter.py
</code></pre></div></div>

<h2 id="how-to-use-mpi-to-accelerate-scatterpy">How to use MPI to accelerate scatter.py</h2>

<p>We have written a partial implementation of the scatter code, which computes incident and scattered fields in parallel. The key changes with respect to the original version are:</p>

<ul>
  <li>The addition of line <code class="language-plaintext highlighter-rouge">from mpi4py import MPI</code> at the top. This will initialise MPI</li>
  <li><code class="language-plaintext highlighter-rouge">comm = MPI.COMM_WORLD</code> gets the communicator</li>
  <li><code class="language-plaintext highlighter-rouge">nprocs = comm.Get_size()</code> gets the total number of processes</li>
  <li><code class="language-plaintext highlighter-rouge">pe = comm.Get_rank()</code> gets the actual rank of a process</li>
  <li>The field computation <code class="language-plaintext highlighter-rouge">computeField(indx)</code> takes index <code class="language-plaintext highlighter-rouge">indx</code> as argument, each index mapping to a different point of the grid. Each process computes the incident and scattered fields for a different set of points in parallel.</li>
  <li>In its current form, the scatter code will not run with option <code class="language-plaintext highlighter-rouge">-save</code>, i.e. not able to save the wave field in files. The field from each process needs to be gathered, see exercise below</li>
</ul>

<blockquote>
  <h2 id="exercises">Exercises</h2>
  <ul>
    <li>implement the field gather operation where all the fields values are assembled onto the root process (required for the <code class="language-plaintext highlighter-rouge">-save</code> option of scatter.py).</li>
    <li>measure parallel execution time. How much faster is the code compared to serial?</li>
  </ul>
</blockquote>

  </div>

  <hr />
  
  

  <div class="PageNavigation">
    
      <a class="prev" href="openmp.html">&laquo; OpenMP</a>
    
    
      <a class="next" href="summary.html">Summary &raquo;</a>
    
  </div>

  
</article>

      </div>
    </main>

    <footer class="site-footer">

  <div class="wrapper">

    <h2 class="footer-heading">New Zealand eScience Infrastructure - Performance Optimisation Training</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li>
            
              New Zealand eScience Infrastructure - Performance Optimisation Training
            
            </li>
            
            <li><a href="mailto:training@nesi.org.nz">training@nesi.org.nz</a></li>
            
        </ul>
      </div>

      <div class="footer-col footer-col-2">
        <ul class="social-media-list">
          

          
          <li>
            <a href="https://twitter.com/nesi_nz"><span class="icon icon--twitter"><svg viewBox="0 0 16 16" width="16px" height="16px"><path fill="#828282" d="M15.969,3.058c-0.586,0.26-1.217,0.436-1.878,0.515c0.675-0.405,1.194-1.045,1.438-1.809c-0.632,0.375-1.332,0.647-2.076,0.793c-0.596-0.636-1.446-1.033-2.387-1.033c-1.806,0-3.27,1.464-3.27,3.27 c0,0.256,0.029,0.506,0.085,0.745C5.163,5.404,2.753,4.102,1.14,2.124C0.859,2.607,0.698,3.168,0.698,3.767 c0,1.134,0.577,2.135,1.455,2.722C1.616,6.472,1.112,6.325,0.671,6.08c0,0.014,0,0.027,0,0.041c0,1.584,1.127,2.906,2.623,3.206 C3.02,9.402,2.731,9.442,2.433,9.442c-0.211,0-0.416-0.021-0.615-0.059c0.416,1.299,1.624,2.245,3.055,2.271 c-1.119,0.877-2.529,1.4-4.061,1.4c-0.264,0-0.524-0.015-0.78-0.046c1.447,0.928,3.166,1.469,5.013,1.469 c6.015,0,9.304-4.983,9.304-9.304c0-0.142-0.003-0.283-0.009-0.423C14.976,4.29,15.531,3.714,15.969,3.058z"/></svg>
</span><span class="username">nesi_nz</span></a>

          </li>
          
        </ul>
      </div>

      <div class="footer-col footer-col-3">
        <p>Training materials for a hands-on workshop on Performance Optimisation
</p>
      </div>
    </div>

    <div class="footer-col"> 

      <img class="navbar-logo" src="../assets/img/NeSI_Icon_Brand_expression_H_RGB.png" alt="NeSI logo" />
    </div>

  </div>

</footer>


  </body>

</html>
